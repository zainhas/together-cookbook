{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Workflows and Recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os, json\n",
    "import together\n",
    "from pydantic import ValidationError\n",
    "from together import AsyncTogether, Together\n",
    "\n",
    "client = Together(api_key= \"abc\")\n",
    "async_client = AsyncTogether(api_key= \"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(user_prompt : str, model : str, system_prompt : str = None):\n",
    "\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,        \n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The capital of the United States of America is Washington, D.C. (short for District of Columbia). It's a federal district that serves as the permanent capital of the country and is not part of any state.\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_llm(\"what is the captial of america?\", model='meta-llama/Llama-3.3-70B-Instruct-Turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The capital of the United States of America (USA) is Washington, D.C. (short for District of Columbia).',\n",
       " 'The capital of the United States of America (USA) is Washington, D.C. (short for District of Columbia).']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function below will call the reference LLMs in parallel\n",
    "async def run_llm_parallel(user_prompt : str, model : str, system_prompt : str = None):\n",
    "    \"\"\"Run parallel LLM calls with a reference model.\"\"\"\n",
    "    for sleep_time in [1, 2, 4]:\n",
    "        try:\n",
    "            messages = []\n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "            messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=2000,\n",
    "            )\n",
    "            break\n",
    "        except together.error.RateLimitError as e:\n",
    "            print(e)\n",
    "            await asyncio.sleep(sleep_time)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Generate intermediate reference responses\n",
    "reference_models = [\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\", \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"]\n",
    "results = await asyncio.gather(*[run_llm_parallel(\"what is the captial of the USA?\", model) for model in reference_models])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSON_llm(user_prompt : str, schema, system_prompt : str = None):\n",
    "    \n",
    "    try:\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        \n",
    "        extract = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            response_format={\n",
    "                \"type\": \"json_object\",\n",
    "                \"schema\": schema.model_json_schema(),\n",
    "            },\n",
    "        )\n",
    "        response = json.loads(extract.choices[0].message.content)\n",
    "\n",
    "    except ValidationError as e:\n",
    "        error_message = f\"Failed to parse JSON: {e}\"\n",
    "        print(error_message)\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'restaurants': [{'name': 'Carbone',\n",
       "   'category': 'Italian-American',\n",
       "   'description': 'Upscale retro Italian-American restaurant in Greenwich Village.'},\n",
       "  {'name': 'Peter Luger Steak House',\n",
       "   'category': 'Steakhouse',\n",
       "   'description': 'Classic steakhouse in Brooklyn, serving top-quality steaks since 1887.'},\n",
       "  {'name': 'Di Fara Pizza',\n",
       "   'category': 'Pizzeria',\n",
       "   'description': 'Classic New York-style pizzeria in Brooklyn.'},\n",
       "  {'name': \"Katz's Delicatessen\",\n",
       "   'category': 'Deli',\n",
       "   'description': 'Classic Jewish deli on the Lower East Side.'},\n",
       "  {'name': 'Eleven Madison Park',\n",
       "   'category': 'Fine dining',\n",
       "   'description': 'Three-Michelin-starred restaurant in the Flatiron District.'},\n",
       "  {'name': 'Xe Lua',\n",
       "   'category': 'Vietnamese',\n",
       "   'description': 'Casual Vietnamese restaurant in Chinatown.'},\n",
       "  {'name': \"Artichoke Basille's Pizza\",\n",
       "   'category': 'Pizzeria',\n",
       "   'description': 'Thick, crispy pizza slices in multiple locations.'},\n",
       "  {'name': 'Le Bernardin',\n",
       "   'category': 'Seafood',\n",
       "   'description': 'Three-Michelin-starred seafood restaurant in Midtown West.'},\n",
       "  {'name': \"Xi'an Famous Foods\",\n",
       "   'category': 'Chinese',\n",
       "   'description': 'Casual noodle shop with hand-pulled noodles.'}]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Restaurant(BaseModel, frozen=True):\n",
    "    name: str\n",
    "    category: str\n",
    "    description: str\n",
    "\n",
    "class RestList(BaseModel, frozen=True):\n",
    "    restaurants : List[Restaurant] = Field(..., default_factory=list)\n",
    "\n",
    "JSON_llm(\"Good restaurants in new york, Output only JSON.\", RestList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prompt chaining\n",
    "\n",
    "`PDF -> Clean text -> Brainstorm and ideation -> Improve and Augment -> Script`\n",
    "\n",
    "2. Routing\n",
    "\n",
    "`Given prompt -> LLM strcutured model choice -> Call this model -> Output`\n",
    "\n",
    "3. Parallelization\n",
    "\n",
    "`Mixture of Agents code`\n",
    "\n",
    "4. Orchestrator-workers\n",
    "\n",
    "`Summarization for product descriptions`\n",
    "\n",
    "5. Evaluator-optimizer\n",
    "\n",
    "`Code generation and evaluation in a loop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Chaining Recipe\n",
    "A simple snippet of serial prompt chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Relevant numerical information: \n",
      "- $12 (hourly wage)\n",
      "- 50 minutes (time worked, can be converted to hours for calculation: 50 / 60 = 5/6 hour)\n",
      "\n",
      "Step 2\n",
      "1. Convert 50 minutes to hours: 50 / 60 = 5/6 hour.\n",
      "2. Multiply the hourly wage by the time worked in hours: $12 * (5/6).\n",
      "3. Calculate the result of the multiplication to find the earnings.\n",
      "\n",
      "Step 3\n",
      "To find the earnings, we need to perform the multiplication of $12 and 5/6.\n",
      "\n",
      "First, convert the fraction to a decimal: 5/6 ≈ 0.83\n",
      "\n",
      "Then, multiply $12 by 0.83: \n",
      "$12 * 0.83 ≈ $9.96\n",
      "\n",
      "So, the earnings are approximately $9.96.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def serial_chain_workflow(input_query: str, prompt_chain : List[str]) -> List[str]:\n",
    "    \"\"\"Run a serial chain of LLM calls to address the `input_query` \n",
    "    using a prompts specified in a list `prompt_chain`.\n",
    "    \"\"\"\n",
    "    response_chain = []\n",
    "    response = input_query\n",
    "    for i, prompt in enumerate(prompt_chain):\n",
    "        print(f\"Step {i+1}\")\n",
    "        response = run_llm(f\"{prompt}\\nInput:\\n{response}\", model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo')\n",
    "        response_chain.append(response)\n",
    "        print(f\"{response}\\n\")\n",
    "    return response_chain\n",
    "\n",
    "# Toy Example\n",
    "\n",
    "question = \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\"\n",
    "\n",
    "prompt_chain = [\"\"\"Given the math problem, ONLY extract any relevant numerical information and how it can be used.\"\"\",\n",
    "                \"\"\"Given the numberical information extracted, ONLY express the steps you would take to solve the problem.\"\"\",\n",
    "                \"\"\"Given the steps, express the final answer to the problem.\"\"\"]\n",
    "\n",
    "responses = serial_chain_workflow(question, prompt_chain)\n",
    "\n",
    "final_answer = responses[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing\n",
    "A simple snippet of the consitional routing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_workflow(input_query: str, routes : Dict[str, str]) -> str:\n",
    "    \"\"\" Given a `input_qeury` and a dictionary of `routes` containing options and details for each.\n",
    "    Selects the best model for the task and return the response from the model.\n",
    "    \"\"\"\n",
    "    ROUTER_PROMPT = \"\"\"Given a user prompt/query: {user_query}, select the best option out of the following routes:\n",
    "    {routes}. Answer only in JSON format.\"\"\"\n",
    "\n",
    "    # Create a schema from the routes dictionary\n",
    "    class Schema(BaseModel):\n",
    "        route: Literal[tuple(routes.keys())]\n",
    "    \n",
    "        reason: str = Field(\n",
    "            description=\"Short one-liner explanation why this route was selected for the task in the prompt/query.\"\n",
    "        )\n",
    "\n",
    "    # Call LLM to select route\n",
    "    selected_route = JSON_llm(ROUTER_PROMPT.format(user_query=input_query, routes=routes), Schema)\n",
    "    print(f\"Selcted route:{selected_route['route']}\\nReason: {selected_route['reason']}\\n\")\n",
    "\n",
    "    # Use LLM on selected route. \n",
    "    # Could also have different prompts that need to be used for each route.\n",
    "    response = run_llm(user_prompt= input_query, model = selected_route['route'])\n",
    "    print(f\"Response: {response}\\n\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Produce python snippet to check to see if a number is prime or not.\n",
      "\n",
      "========================================\n",
      "Seelction route:Qwen/Qwen2.5-Coder-32B-Instruct\n",
      " Reason: The task requires generating a Python code snippet to check if a number is prime or not, which falls under code generation tasks.\n",
      "\n",
      "Response: Certainly! Below is a Python function that checks whether a given number is prime or not:\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    \"\"\"Check if a number is prime.\"\"\"\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "    i = 5\n",
      "    while i * i <= n:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "\n",
      "# Example usage:\n",
      "number = 29\n",
      "if is_prime(number):\n",
      "    print(f\"{number} is a prime number.\")\n",
      "else:\n",
      "    print(f\"{number} is not a prime number.\")\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Initial Checks**: \n",
      "   - Numbers less than or equal to 1 are not prime.\n",
      "   - Numbers 2 and 3 are prime.\n",
      "   \n",
      "2. **Divisibility Check**:\n",
      "   - If the number is divisible by 2 or 3, it is not prime.\n",
      "   \n",
      "3. **Loop through potential factors**:\n",
      "   - Starting from 5, check divisibility up to the square root of `n`.\n",
      "   - The loop increments by 6 each time, checking `i` and `i + 2` (i.e., 5 and 7, 11 and 13, etc.), because all primes greater than 3 can be written in the form of 6k ± 1.\n",
      "\n",
      "This method is efficient for checking primality for reasonably large numbers.\n",
      "\n",
      "Task 2: Plan and provide a short itenary for a 2 week vacation in Europe.\n",
      "\n",
      "========================================\n",
      "Seelction route:Qwen/QwQ-32B-Preview\n",
      " Reason: Planning and providing a short itinerary for a 2-week vacation in Europe requires reasoning, planning, and multi-step tasks.\n",
      "\n",
      "Response: Planning a 2-week vacation in Europe can be both exciting and overwhelming due to the vast number of options available. Europe is rich in history, culture, and natural beauty, offering something for every type of traveler. To make the most of your time, it's essential to prioritize the destinations that interest you the most and plan a balanced itinerary that allows for both exploration and relaxation.\n",
      "\n",
      "### Step-by-Step Planning Guide\n",
      "\n",
      "1. **Define Your Interests:**\n",
      "   - History and Culture: Museums, castles, historical sites.\n",
      "   - Nature and Outdoors: Mountains, beaches, national parks.\n",
      "   - Food and Wine: Culinary experiences, wine tastings.\n",
      "   - City Life: Shopping, nightlife, contemporary culture.\n",
      "\n",
      "2. **Choose Countries and Cities:**\n",
      "   - Consider visa requirements, language barriers, and travel time between destinations.\n",
      "   - Popular choices include France, Italy, Spain, Germany, the UK, and the Netherlands.\n",
      "\n",
      "3. **Plan Your Route:**\n",
      "   - Start from the main entry point (e.g., London, Paris, Amsterdam).\n",
      "   - Plan a route that minimizes backtracking and efficient travel between cities.\n",
      "   - Consider internal flights or train routes for faster travel.\n",
      "\n",
      "4. **Allocate Time per Destination:**\n",
      "   - Spend at least 2-3 days in each city to explore adequately.\n",
      "   - Allow for travel days between destinations.\n",
      "\n",
      "5. **Book Accommodations:**\n",
      "   - Choose accommodations based on location, budget, and preferences.\n",
      "   - Book in advance, especially during peak seasons.\n",
      "\n",
      "6. **Research Attractions and Activities:**\n",
      "   - Make a list of must-see sights and experiences in each destination.\n",
      "   - Consider purchasing tickets in advance for popular attractions.\n",
      "\n",
      "7. **Budgeting:**\n",
      "   - Estimate costs for flights, accommodations, transportation, food, and activities.\n",
      "   - Set a daily budget to manage expenses.\n",
      "\n",
      "8. **Pack Smart:**\n",
      "   - Check the weather forecast for your destinations.\n",
      "   - Pack versatile clothing and comfortable shoes.\n",
      "   - Bring essential travel documents and copies.\n",
      "\n",
      "### Sample Itinerary\n",
      "\n",
      "#### Week 1: France and Italy\n",
      "\n",
      "**Day 1-3: Paris, France**\n",
      "- Arrival in Paris.\n",
      "- Visit the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\n",
      "- Stroll along the Champs-Élysées and enjoy French cuisine.\n",
      "\n",
      "**Day 4-6: Florence, Italy**\n",
      "- Fly from Paris to Florence.\n",
      "- Explore the Uffizi Gallery, Ponte Vecchio, and the Duomo.\n",
      "- Take a day trip to Tuscany for vineyard tours.\n",
      "\n",
      "**Day 7-9: Rome, Italy**\n",
      "- Travel from Florence to Rome by train.\n",
      "- Visit the Colosseum, Roman Forum, and Vatican City.\n",
      "- Enjoy pizza and gelato in Trastevere.\n",
      "\n",
      "#### Week 2: Spain and Portugal\n",
      "\n",
      "**Day 10-12: Barcelona, Spain**\n",
      "- Fly from Rome to Barcelona.\n",
      "- See Gaudí’s Sagrada Família, Park Güell, and Casa Batlló.\n",
      "- Relax on the beaches of Barceloneta.\n",
      "\n",
      "**Day 13-15: Lisbon, Portugal**\n",
      "- Travel from Barcelona to Lisbon by train or flight.\n",
      "- Explore Belem Tower, Jerónimos Monastery, and Alfama district.\n",
      "- Take a tram to Cristo Rei and enjoy Fado music.\n",
      "\n",
      "**Day 16: Departure**\n",
      "- Transfer to the airport for your return flight.\n",
      "\n",
      "### Tips for a Smooth Trip\n",
      "\n",
      "- **Travel Light:** Carry a backpack or rolling suitcase for ease of movement.\n",
      "- **Stay Connected:** Consider getting a local SIM card or international data plan.\n",
      "- **Learn Basic Phrases:** Knowing a few words in the local language can be helpful.\n",
      "- **Stay Flexible:** Be prepared for changes in plans due to weather or other factors.\n",
      "- **Enjoy the Journey:** Take time to soak in the culture and meet new people.\n",
      "\n",
      "Europe is a continent filled with diverse experiences, and a well-planned itinerary can ensure that you make the most of your two-week adventure. Whether you're exploring historic cities, indulging in local cuisine, or relaxing in natural美景, Europe has something to offer every traveler.\n",
      "\n",
      "Task 3: Write a short story about a dragon and a knight.\n",
      "\n",
      "========================================\n",
      "Seelction route:Gryphe/MythoMax-L2-13b\n",
      " Reason: The task requires story-telling and fantasy, which is best suited for the Gryphe/MythoMax-L2-13b model.\n",
      "\n",
      "Response: \n",
      " In a faraway kingdom, there once lived a mighty dragon named Drako. He was known for his fiery breath and sharp claws, terrorizing the nearby villages and towns. The king of the land decided to send his bravest knight, Sir William, to slay the beast and bring peace to his people.\n",
      "Sir William set out on his quest, armed with his sword and shield. After days of riding through the dense forest, he finally saw the dragon's lair. The knight dismounted from his horse and slowly approached the cave entrance. He took a deep breath, bracing himself for the battle ahead.\n",
      "As he stepped into the dark cave, he heard a loud roar that sent shivers down his spine. The dragon emerged from the shadows, its fiery eyes fixed on the knight. The beast let out another deafening roar, and Sir William readied his sword.\n",
      "What happened next surprised everyone. Instead of attacking, Drako spoke to the knight. He told him about how lonely he had been, how he had been driven to terrorize the kingdom out of desperation. Sir William listened intently, realizing that the dragon was not inherently evil but rather misunderstood.\n",
      "The knight proposed a truce between them, offering to help Drako find a new home and food if he promised to stop terrorizing the towns. Drako agreed, and together they left the cave.\n",
      "The people of the kingdom were amazed to see the dragon and the knight walking side by side. Sir William explained what had happened, and the king agreed to the truce. Drako was given a new home in a remote part of the kingdom, far away from the towns, and he was provided with food to sustain him.\n",
      "From that day forward, the dragon and the knight became unlikely friends. They would often visit each other, and the people of the kingdom knew that they were safe from harm. The legend of the dragon and the knight lived on, teaching everyone that even the fiercest of enemies could become friends if given the chance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_list = [\"Produce python snippet to check to see if a number is prime or not.\",\n",
    "               \"Plan and provide a short itenary for a 2 week vacation in Europe.\",\n",
    "               \"Write a short story about a dragon and a knight.\"]\n",
    "\n",
    "model_routes = {\n",
    "    \"Qwen/Qwen2.5-Coder-32B-Instruct\" : \"Best model choice for code generation tasks.\",\n",
    "    \"Gryphe/MythoMax-L2-13b\" : \"Best model choice for story-telling, role-playing and fantasy tasks.\",\n",
    "    \"Qwen/QwQ-32B-Preview\" : \"Best model for reasoning, planning and muilti-step tasks\",\n",
    "}\n",
    "\n",
    "for i, prompt in enumerate(prompt_list):\n",
    "    print(f\"Task {i+1}: {prompt}\\n\")\n",
    "    print(20*'==')\n",
    "    router_workflow(prompt, model_routes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Recipe\n",
    "A simple snippet of parallel agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parallel_workflow(prompt : str, proposer_models : List[str], aggregator_model : str, aggregator_prompt: str):\n",
    "    \"\"\"Run a parallel chain of LLM calls to address the `input_query` \n",
    "    using a list of models specified in `models`.\n",
    "\n",
    "    Returns output from final aggregator model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Gather intermediate responses from proposer models\n",
    "    proposed_responses = await asyncio.gather(*[run_llm_parallel(prompt, model) for model in proposer_models])\n",
    "    \n",
    "    # Aggregate responses using an aggregator model\n",
    "    final_output = run_llm(user_prompt=prompt,\n",
    "                           model=aggregator_model,\n",
    "                           system_prompt=aggregator_prompt + \"\\n\" + \"\\n\".join(f\"{i+1}. {str(element)}\" for i, element in enumerate(proposed_responses)\n",
    "           ))\n",
    "    \n",
    "    return final_output, proposed_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_models = [\n",
    "    \"microsoft/WizardLM-2-8x22B\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "]\n",
    "\n",
    "user_prompt = \"\"\"Jenna and her mother picked some apples from their apple farm. \n",
    "Jenna picked half as many apples as her mom. If her mom got 20 apples, how many apples did they both pick?\"\"\"\n",
    "\n",
    "aggregator_model = \"deepseek-ai/DeepSeek-V3\"\n",
    "\n",
    "aggregator_system_prompt = \"\"\"You have been provided with a set of responses from various open-source models to the latest user query.\n",
    "Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information\n",
    "provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the\n",
    "given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured,\n",
    "coherent, and adheres to the highest standards of accuracy and reliability.\n",
    "\n",
    "Responses from models:\"\"\"\n",
    "\n",
    "answer, intermediate_reponses = await parallel_workflow(prompt = user_prompt, \n",
    "                                                        proposer_models = reference_models, \n",
    "                                                        aggregator_model = aggregator_model, \n",
    "                                                        aggregator_prompt = aggregator_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermetidate Response 1:\n",
      "\n",
      " Let's think step by step.To solve the problem, we need to determine the total number of apples picked by Jenna and her mother combined. We are given two pieces of information:\n",
      "\n",
      "1. Jenna's mother picked 20 apples.\n",
      "2. Jenna picked half as many apples as her mother.\n",
      "\n",
      "Let's break down the solution process:\n",
      "\n",
      "Step 1: Determine the number of apples Jenna's mother picked.\n",
      "- According to the information provided, Jenna's mother picked 20 apples.\n",
      "\n",
      "Step 2: Calculate the number of apples Jenna picked.\n",
      "- Since Jenna picked half as many apples as her mother, we need to find half of the mother's count.\n",
      "- Half of 20 apples is calculated by dividing 20 by 2, which gives us 10 apples.\n",
      "- Therefore, Jenna picked 10 apples.\n",
      "\n",
      "Step 3: Find the total number of apples they both picked.\n",
      "- To find the total, we add the number of apples picked by Jenna to the number of apples picked by her mother.\n",
      "- Adding Jenna's 10 apples to her mother's 20 apples gives us a total of 30 apples.\n",
      "\n",
      "Step 4: State the final answer.\n",
      "- The total number of apples picked by Jenna and her mother is 30.\n",
      "\n",
      "Therefore, the final answer to the question is \\boxed{30}.\n",
      "\n",
      "The answer is: 30.\n",
      "\n",
      "Intermetidate Response 2:\n",
      "\n",
      "If Jenna's mother picked 20 apples and Jenna picked half as many apples as her mother, then Jenna picked:\n",
      "\n",
      "\\[ \\frac{1}{2} \\times 20 = 10 \\text{ apples} \\]\n",
      "\n",
      "Together, they picked:\n",
      "\n",
      "\\[ 20 + 10 = 30 \\text{ apples} \\]\n",
      "\n",
      "So, Jenna and her mother picked a total of 30 apples.\n",
      "\n",
      "Intermetidate Response 3:\n",
      "\n",
      "Here's how to solve the problem:\n",
      "\n",
      "* **Jenna's apples:** Jenna picked half the amount her mom did, so she picked 20 / 2 = 10 apples.\n",
      "* **Total apples:** Together they picked 20 + 10 = 30 apples.\n",
      "\n",
      "\n",
      "**Answer:** They picked a total of 30 apples. \n",
      "\n",
      "\n",
      "Intermetidate Response 4:\n",
      "\n",
      "To find the total number of apples picked, we need to find out how many Jenna picked, then add that to the number her mom picked. \n",
      "\n",
      "Since Jenna picked half as many apples as her mom, and her mom picked 20 apples, Jenna picked 20 / 2 = 10 apples.\n",
      "\n",
      "So, Jenna's mom picked 20 apples and Jenna picked 10 apples. \n",
      "\n",
      "The total number of apples picked is 20 + 10 = 30 apples. The answer is 30.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, response in enumerate(intermediate_reponses):\n",
    "    print(f\"Intermetidate Response {i+1}:\\n\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: To determine the total number of apples Jenna and her mother picked together, follow these steps:\n",
      "\n",
      "1. **Determine the number of apples Jenna's mother picked:**\n",
      "   - Jenna's mother picked **20 apples**.\n",
      "\n",
      "2. **Calculate the number of apples Jenna picked:**\n",
      "   - Jenna picked half as many apples as her mother.\n",
      "   - Half of 20 is \\( \\frac{1}{2} \\times 20 = 10 \\) apples.\n",
      "   - So, Jenna picked **10 apples**.\n",
      "\n",
      "3. **Find the total number of apples they both picked:**\n",
      "   - Add the number of apples picked by Jenna and her mother: \\( 20 + 10 = 30 \\) apples.\n",
      "\n",
      "**Final Answer:** Jenna and her mother picked a total of **30 apples**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator Agent Workflow\n",
    "A simple snippet of the parallel orchestrator-worker agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List\n",
    "\n",
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "Analyze this task and break it down into 2-3 distinct approaches:\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Provide an Analysis:\n",
    "\n",
    "Explain your understanding of the task and which variations would be valuable.\n",
    "Focus on how each approach serves different aspects of the task.\n",
    "\n",
    "Along with the analysis, provide 2-3 approaches to tackle the task, each with a brief description:\n",
    "\n",
    "Formal style: Write technically and precisely, focusing on detailed specifications\n",
    "Conversational style: Write in a friendly and engaging way that connects with the reader\n",
    "Hybrid style: Tell a story that includes technical details, combining emotional elements with specifications\n",
    "\n",
    "Return only JSON output.\n",
    "\"\"\"\n",
    "\n",
    "WORKER_PROMPT = \"\"\"\n",
    "Generate content based on:\n",
    "Task: {original_task}\n",
    "Style: {task_type}\n",
    "Guidelines: {task_description}\n",
    "\n",
    "Return only your response:\n",
    "[Your content here, maintaining the specified style and fully addressing requirements.]\n",
    "\"\"\"\n",
    "\n",
    "task = \"\"\"Write a product description for a new eco-friendly water bottle.\n",
    "The target_audience is environmentally conscious millennials and key product features are: plastic-free, insulated, lifetime warranty\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    type: Literal[\"formal\", \"conversational\", \"hybrid\"]\n",
    "    description: str\n",
    "\n",
    "class TaskList(BaseModel):\n",
    "    analysis: str\n",
    "    tasks: List[Task]  = Field(..., default_factory=list)\n",
    "\n",
    "async def orchestrator_workflow(task : str, orchestrator_prompt : str, worker_prompt : str): \n",
    "    \"\"\"Use a orchestrator model to break down a task into sub-tasks and then use worker models to generate and return responses.\"\"\"\n",
    "\n",
    "    # Use orchestrator model to break the task up into sub-tasks\n",
    "    orchestrator_response = JSON_llm(orchestrator_prompt.format(task=task), schema=TaskList)\n",
    " \n",
    "    # Parse orchestrator response\n",
    "    analysis = orchestrator_response[\"analysis\"]\n",
    "    tasks= orchestrator_response[\"tasks\"]\n",
    "\n",
    "    print(\"\\n=== ORCHESTRATOR OUTPUT ===\")\n",
    "    print(f\"\\nANALYSIS:\\n{analysis}\")\n",
    "    print(f\"\\nTASKS:\\n{json.dumps(tasks, indent=2)}\")\n",
    "\n",
    "    worker_model =  [\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"]*len(tasks)\n",
    "\n",
    "    # Gather intermediate responses from worker models\n",
    "    return tasks , await asyncio.gather(*[run_llm_parallel(user_prompt=worker_prompt.format(original_task=task, task_type=task_info['type'], task_description=task_info['description']), model=model) for task_info, model in zip(tasks,worker_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ORCHESTRATOR OUTPUT ===\n",
      "\n",
      "ANALYSIS:\n",
      "The task requires writing a product description for an eco-friendly water bottle targeting environmentally conscious millennials. The key features to highlight are the plastic-free material, insulated design, and lifetime warranty. A valuable product description should effectively communicate these features while resonating with the target audience.\n",
      "\n",
      "TASKS:\n",
      "[\n",
      "  {\n",
      "    \"type\": \"formal\",\n",
      "    \"description\": \"Write a technically precise product description focusing on detailed specifications, such as the materials used, insulation technology, and warranty terms. This approach serves the task by providing a clear understanding of the product's features and benefits.\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"conversational\",\n",
      "    \"description\": \"Write a friendly and engaging product description that connects with the reader on an emotional level. This approach serves the task by building a relationship with the target audience and highlighting the product's eco-friendly aspects in a relatable way.\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"hybrid\",\n",
      "    \"description\": \"Tell a story that incorporates technical details, combining emotional elements with specifications. This approach serves the task by creating an immersive experience for the reader, highlighting the product's features, and emphasizing its eco-friendly benefits in a compelling narrative.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "task = \"\"\"Write a product description for a new eco-friendly water bottle. \n",
    "The target_audience is environmentally conscious millennials and key product features are: plastic-free, insulated, lifetime warranty\n",
    "\"\"\"\n",
    "\n",
    "tasks, worker_resp = await orchestrator_workflow(task, orchestrator_prompt=ORCHESTRATOR_PROMPT, worker_prompt=WORKER_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WORKER RESULT (formal) ===\n",
      "Introduction to the HydraGreen Water Bottle\n",
      "\n",
      "We are pleased to introduce the HydraGreen water bottle, a revolutionary, eco-friendly hydration solution designed specifically for environmentally conscious millennials. This premium product boasts a plastic-free construction, advanced insulation technology, and a comprehensive lifetime warranty, ensuring a superior user experience while minimizing its ecological footprint.\n",
      "\n",
      "Materials and Construction\n",
      "\n",
      "The HydraGreen water bottle is crafted from high-quality, BPA-free stainless steel (18/8 food-grade) and features a durable, non-toxic silicone sleeve. The bottle's body is constructed using a proprietary double-walled insulation process, which provides exceptional thermal retention while maintaining a slim, ergonomic design. The lid is manufactured from a sustainable, plant-based polymer, further reducing the product's reliance on petroleum-derived materials.\n",
      "\n",
      "Insulation Technology\n",
      "\n",
      "The HydraGreen water bottle incorporates our patented ThermaLock insulation system, which leverages the principles of vacuum insulation to minimize heat transfer. This advanced technology enables the bottle to maintain temperatures for an extended period, keeping beverages hot for up to 12 hours or cold for up to 24 hours. The ThermaLock system consists of a evacuated insulation space, a high-performance insulation material, and a precision-engineered lid seal, ensuring optimal thermal performance.\n",
      "\n",
      "Lifetime Warranty\n",
      "\n",
      "The HydraGreen water bottle is backed by a comprehensive lifetime warranty, providing users with unparalleled peace of mind. The warranty covers all defects in materials and workmanship, including the insulation system, lid, and bottle body. In the unlikely event of a defect, our dedicated customer service team will promptly replace or repair the product, free of charge. The warranty is valid for the lifetime of the product, with no expiration date or limitations on the number of claims.\n",
      "\n",
      "Specifications\n",
      "\n",
      "- Material: 18/8 food-grade stainless steel, BPA-free silicone, and plant-based polymer\n",
      "- Insulation: Patented ThermaLock vacuum insulation system\n",
      "- Capacity: 27 oz (800 ml)\n",
      "- Dimensions: 10.5 in (26.7 cm) x 3.5 in (8.9 cm)\n",
      "- Weight: 13.5 oz (383 g)\n",
      "- Temperature range: -20°C to 100°C (-4°F to 212°F)\n",
      "- Warranty: Lifetime warranty, covering defects in materials and workmanship\n",
      "\n",
      "Conclusion\n",
      "\n",
      "The HydraGreen water bottle is an exemplary model of innovative, eco-friendly design, combining exceptional performance, durability, and sustainability. With its plastic-free construction, advanced insulation technology, and comprehensive lifetime warranty, this product is poised to become an indispensable companion for environmentally conscious millennials seeking a reliable, high-quality hydration solution.\n",
      "\n",
      "\n",
      "=== WORKER RESULT (conversational) ===\n",
      "Meet your new favorite companion - our eco-friendly water bottle that's as passionate about the planet as you are. As someone who cares deeply about the environment, you're probably tired of using plastic water bottles that end up in landfills and oceans. That's why we created a game-changing alternative that's not only better for the Earth, but also keeps your drinks hot or cold for hours.\n",
      "\n",
      "Our bottle is made from sustainable, plastic-free materials that are designed to last a lifetime. And we mean that literally - we're so confident in the quality of our product that we're backing it with a lifetime warranty. Whether you're hiking, biking, or just running errands, our insulated bottle will keep your drinks at the perfect temperature, and its durable design will withstand even the toughest adventures.\n",
      "\n",
      "But here's the thing: this bottle is about so much more than just keeping your drinks hot or cold. It's about making a statement. It's about saying that you care about the planet, and you're willing to take small steps every day to reduce your impact. By choosing our eco-friendly water bottle, you're joining a community of like-minded individuals who are passionate about creating a more sustainable future.\n",
      "\n",
      "So why settle for a flimsy, plastic water bottle that's just going to end up in a landfill? Choose a bottle that aligns with your values, and that will be your trusted sidekick for years to come. Join the movement towards a more sustainable future, and experience the difference that our eco-friendly water bottle can make.\n",
      "\n",
      "\n",
      "=== WORKER RESULT (hybrid) ===\n",
      "Imagine yourself hiking through a lush forest, the warm sun beating down on your skin, and the sound of a gentle stream babbling in the distance. As you walk, you can't help but feel a deep connection to the natural world around you. But with that connection comes a sense of responsibility – a responsibility to protect the planet and preserve its beauty for future generations.\n",
      "\n",
      "That's where our new eco-friendly water bottle comes in. Made from 100% plastic-free materials, this innovative bottle is not only a stylish accessory but also a powerful tool in the fight against plastic waste. The insulated design keeps your drinks hot or cold for hours, whether you're sipping coffee on a chilly morning or enjoying a refreshing glass of water on a sweltering summer day.\n",
      "\n",
      "But what really sets our bottle apart is its lifetime warranty. We're so confident in the quality and durability of our product that we're willing to stand behind it for life. That means you can enjoy your favorite beverages without worrying about the environmental impact of constantly replacing disposable bottles. And with our bottle's sleek, modern design, you'll be proud to show it off to friends and family.\n",
      "\n",
      "But the story doesn't end there. Every time you use our eco-friendly water bottle, you're making a statement. You're saying that you care about the planet and its inhabitants, and that you're willing to take action to protect them. You're joining a community of like-minded individuals who are committed to reducing their plastic footprint and living a more sustainable lifestyle.\n",
      "\n",
      "So why settle for a boring, disposable water bottle when you can have a stylish, eco-friendly one that makes a real difference? Our plastic-free, insulated bottle is the perfect choice for anyone who wants to stay hydrated while also doing their part for the planet. With its lifetime warranty and sleek design, it's an investment that will pay off for years to come.\n",
      "\n",
      "Technical specifications:\n",
      "\n",
      "* Material: 100% plastic-free, BPA-free, and phthalate-free\n",
      "* Insulation: Double-walled insulation keeps drinks hot or cold for hours\n",
      "* Warranty: Lifetime warranty against defects and damage\n",
      "* Capacity: 27 oz (800 ml)\n",
      "* Dimensions: 10.5 inches (26.7 cm) tall, 3.5 inches (8.9 cm) wide\n",
      "* Weight: 1.2 pounds (0.5 kg)\n",
      "\n",
      "Join the movement towards a more sustainable future. Choose our eco-friendly water bottle and experience the perfect blend of style, functionality, and environmental responsibility.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_info, response in zip(tasks, worker_resp):\n",
    "    print(f\"\\n=== WORKER RESULT ({task_info['type']}) ===\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Optimizer Agent Workflow\n",
    "A simple snippet of looping generator-evaluator workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Implement a Stack with:\n",
    "1. push(x)\n",
    "2. pop()\n",
    "3. getMin()\n",
    "All operations should be O(1).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_PROMPT = \"\"\"\n",
    "Your goal is to complete the task based on <user input>. If there are feedback \n",
    "from your previous generations, you should reflect on them to improve your solution\n",
    "\n",
    "Output your answer concisely in the following format: \n",
    "\n",
    "Thoughts:\n",
    "[Your understanding of the task and feedback and how you plan to improve]\n",
    "\n",
    "Response:\n",
    "[Your code implementation here]\n",
    "\"\"\"\n",
    "\n",
    "def generate(task: str, generator_prompt: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    full_prompt = f\"{generator_prompt}\\n{context}\\nTask: {task}\" if context else f\"{generator_prompt}\\nTask: {task}\"\n",
    "\n",
    "    response = run_llm(full_prompt, model=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "    \n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "    print(f\"Output:\\n{response}\\n\")\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "EVALUATOR_PROMPT = \"\"\"\n",
    "Evaluate this following code implementation for:\n",
    "1. code correctness\n",
    "2. time complexity\n",
    "3. style and best practices\n",
    "\n",
    "You should be evaluating only and not attemping to solve the task.\n",
    "\n",
    "Only output \"PASS\" if all criteria are met and you have no further suggestions for improvements.\n",
    "\n",
    "Provide detailed feedback if there are areas that need improvement. You should specify what needs improvement and why.\n",
    "\n",
    "Only output JSON.\n",
    "\"\"\"\n",
    "\n",
    "def evaluate(task : str, evaluator_prompt : str, generated_content: str, schema) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    full_prompt = f\"{evaluator_prompt}\\nOriginal task: {task}\\nContent to evaluate: {generated_content}\"\n",
    "    \n",
    "    response = JSON_llm(full_prompt, schema)\n",
    "    \n",
    "    evaluation = response[\"evaluation\"]\n",
    "    feedback = response[\"feedback\"]\n",
    "\n",
    "    print(\"=== EVALUATION START ===\")\n",
    "    print(f\"Status: {evaluation}\")\n",
    "    print(f\"Feedback: {feedback}\")\n",
    "    print(\"=== EVALUATION END ===\\n\")\n",
    "\n",
    "    return evaluation, feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_workflow(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until the evaluator passes the last generated response.\"\"\"\n",
    "    # Store previous responses from generator\n",
    "    memory = []\n",
    "    \n",
    "    # Generate initial response\n",
    "    response = generate(task, generator_prompt)\n",
    "    memory.append(response)\n",
    "\n",
    "    #Build a schema for the evaluation\n",
    "    class Evaluation(BaseModel):\n",
    "        evaluation: Literal[\"PASS\", \"NEEDS_IMPROVEMENT\", \"FAIL\"]\n",
    "        feedback: str\n",
    "\n",
    "    # While the generated response is not passing, keep generating and evaluating\n",
    "    while True:\n",
    "        evaluation, feedback = evaluate(task, evaluator_prompt, response, Evaluation)\n",
    "        # Terminating condition\n",
    "        if evaluation == \"PASS\":\n",
    "            return response\n",
    "        \n",
    "        # Add current response and feedback to context and generate a new response\n",
    "        context = \"\\n\".join([\n",
    "            \"Previous attempts:\",\n",
    "            *[f\"- {m}\" for m in memory],\n",
    "            f\"\\nFeedback: {feedback}\"\n",
    "        ])\n",
    "        \n",
    "        response = generate(generator_prompt, task, context)\n",
    "        memory.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "To implement a stack with the operations `push(x)`, `pop()`, and `getMin()` all in O(1) time complexity, we can use two stacks. One stack will store the actual stack elements, and the other will store the minimum values. The minimum stack will help us keep track of the minimum element efficiently.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        if self.stack:\n",
      "            return self.stack[-1]\n",
      "        raise IndexError(\"Stack is empty\")\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        if self.min_stack:\n",
      "            return self.min_stack[-1]\n",
      "        raise IndexError(\"Stack is empty\")\n",
      "\n",
      "# Example usage:\n",
      "# min_stack = MinStack()\n",
      "# min_stack.push(-2)\n",
      "# min_stack.push(0)\n",
      "# min_stack.push(-3)\n",
      "# print(min_stack.getMin())  # Returns -3\n",
      "# min_stack.pop()\n",
      "# print(min_stack.top())     # Returns 0\n",
      "# print(min_stack.getMin())  # Returns -2\n",
      "```\n",
      "\n",
      "This implementation ensures that all operations (`push`, `pop`, and `getMin`) are performed in O(1) time complexity. The `min_stack` keeps track of the minimum elements, allowing us to retrieve the minimum element in constant time.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: The code is mostly correct, but there are a few issues and areas for improvement.\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The previous implementation is mostly correct and efficient, ensuring that all operations (`push`, `pop`, and `getMin()`) are performed in O(1) time complexity. However, we can make some improvements to enhance readability and handle edge cases more gracefully. Specifically, we should ensure that the `top()` method does not raise an exception when the stack is empty, as it is not a typical expectation for a stack's `top()` method to raise an exception in all cases. Instead, we can return `None` or a suitable message indicating that the stack is empty.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        if self.stack:\n",
      "            return self.stack[-1]\n",
      "        return None  # Return None if the stack is empty\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        if self.min_stack:\n",
      "            return self.min_stack[-1]\n",
      "        return None  # Return None if the stack is empty\n",
      "\n",
      "# Example usage:\n",
      "# min_stack = MinStack()\n",
      "# min_stack.push(-2)\n",
      "# min_stack.push(0)\n",
      "# min_stack.push(-3)\n",
      "# print(min_stack.getMin())  # Returns -3\n",
      "# min_stack.pop()\n",
      "# print(min_stack.top())     # Returns 0\n",
      "# print(min_stack.getMin())  # Returns -2\n",
      "```\n",
      "\n",
      "This updated implementation handles empty stack scenarios more gracefully by returning `None` from the `top()` and `getMin()` methods when the stack is empty, rather than raising an exception.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: Code Correctness: PASS, Time Complexity: PASS, Style and Best Practices: FAIL, improvements needed: \n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The current implementation is correct in terms of functionality and time complexity. However, the feedback suggests improving style and best practices. Specifically, we should ensure consistent return types and handle edge cases gracefully. Returning `None` for empty stack scenarios is a good practice, but we can also add docstrings to improve code readability and maintainability.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the stack and the minimum stack.\"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        \"\"\"Push element x onto the stack.\"\"\"\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        \"\"\"Remove the element on top of the stack.\"\"\"\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        \"\"\"Get the top element of the stack.\"\"\"\n",
      "        return self.stack[-1] if self.stack else None\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        \"\"\"Retrieve the minimum element in the stack.\"\"\"\n",
      "        return self.min_stack[-1] if self.min_stack else None\n",
      "\n",
      "# Example usage:\n",
      "# min_stack = MinStack()\n",
      "# min_stack.push(-2)\n",
      "# min_stack.push(0)\n",
      "# min_stack.push(-3)\n",
      "# print(min_stack.getMin())  # Returns -3\n",
      "# min_stack.pop()\n",
      "# print(min_stack.top())     # Returns 0\n",
      "# print(min_stack.getMin())  # Returns -2\n",
      "```\n",
      "\n",
      "This updated implementation includes docstrings for each method to improve code readability and maintainability, while still adhering to the requirement of handling empty stack scenarios gracefully by returning `None`.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: code_correctness: PASS, time_complexity: PASS, style_and_best_practices: FAIL, suggestions: {\"1. Error Handling\": \"The current implementation does not handle potential errors that may occur during execution. For instance, the pop method does not check if the stack is empty before attempting to remove an element, which can lead to an IndexError. It is essential to add error handling mechanisms to ensure the code behaves as expected in different scenarios.\", \"2. Type Hints\": \"Although type hints are provided for the push method, they are missing for the pop, top, and getMin methods. Consistent use of type hints can improve code readability and maintainability.\", \"3. Docstrings\": \"While docstrings are provided for each method, they do not follow the Google Python Style Guide, which is a widely accepted standard for Python documentation. It is recommended to adhere to this style guide for consistency and readability.\", \"4. Redundant Code\": \"The getMin method checks if the min_stack is empty before returning the minimum element. However, this check is redundant since the push method ensures that the min_stack is updated accordingly. The getMin method can be simplified by removing this check.\", \"5. Method Naming\": \"The method names are clear and descriptive. However, the name 'top' is not explicitly mentioned in the problem statement. Consider renaming it to something more descriptive, such as 'get_top_element'.\", \"6. Return Type Consistency\": \"The return types of the methods are not consistent. The push and pop methods return None, while the top and getMin methods return the top element and the minimum element, respectively, or None if the stack is empty. It is essential to maintain consistent return types throughout the class.\", \"7. Example Usage\": \"The example usage is not part of the class implementation and should be separated from the class definition. Consider adding it as a separate test suite or in a __main__ block.\"}\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The feedback highlights several areas for improvement in the `MinStack` implementation, including error handling, type hints, docstring formatting, method naming, return type consistency, and example usage separation. I will address these points to enhance the code's robustness and adherence to best practices.\n",
      "\n",
      "1. **Error Handling**: Ensure that the `pop` method does not raise an `IndexError` if the stack is empty.\n",
      "2. **Type Hints**: Add type hints to all methods for better readability and maintainability.\n",
      "3. **Docstrings**: Follow the Google Python Style Guide for consistency.\n",
      "4. **Redundant Code**: Simplify the `getMin` method by removing unnecessary checks.\n",
      "5. **Method Naming**: Rename `top` to `get_top_element` for clarity.\n",
      "6. **Return Type Consistency**: Ensure that the return types are consistent across methods.\n",
      "7. **Example Usage**: Separate the example usage into a `__main__` block.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the stack and the minimum stack.\"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        \"\"\"Push element x onto the stack.\n",
      "\n",
      "        Args:\n",
      "            x (int): The element to be pushed onto the stack.\n",
      "        \"\"\"\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        \"\"\"Remove the element on top of the stack.\n",
      "\n",
      "        Raises:\n",
      "            IndexError: If the stack is empty.\n",
      "        \"\"\"\n",
      "        if not self.stack:\n",
      "            raise IndexError(\"pop from empty stack\")\n",
      "        x = self.stack.pop()\n",
      "        if x == self.min_stack[-1]:\n",
      "            self.min_stack.pop()\n",
      "\n",
      "    def get_top_element(self) -> int:\n",
      "        \"\"\"Get the top element of the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The top element of the stack.\n",
      "\n",
      "        Raises:\n",
      "            IndexError: If the stack is empty.\n",
      "        \"\"\"\n",
      "        if not self.stack:\n",
      "            raise IndexError(\"top from empty stack\")\n",
      "        return self.stack[-1]\n",
      "\n",
      "    def get_min(self) -> int:\n",
      "        \"\"\"Retrieve the minimum element in the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The minimum element in the stack.\n",
      "\n",
      "        Raises:\n",
      "            IndexError: If the stack is empty.\n",
      "        \"\"\"\n",
      "        if not self.min_stack:\n",
      "            raise IndexError(\"get_min from empty stack\")\n",
      "        return self.min_stack[-1]\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Example usage:\n",
      "    min_stack = MinStack()\n",
      "    min_stack.push(-2)\n",
      "    min_stack.push(0)\n",
      "    min_stack.push(-3)\n",
      "    print(min_stack.get_min())  # Returns -3\n",
      "    min_stack.pop()\n",
      "    print(min_stack.get_top_element())     # Returns 0\n",
      "    print(min_stack.get_min())  # Returns -2\n",
      "```\n",
      "\n",
      "This updated implementation addresses all the feedback points, ensuring that the code is robust, follows best practices, and is easy to understand and maintain.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: The provided implementation meets most of the criteria but requires some adjustments for improvement.\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The previous implementation is mostly correct but can be improved by handling empty stack scenarios more gracefully, ensuring consistent return types, and providing clear documentation. Specifically, we should return `None` instead of raising an `IndexError` for empty stack operations, and we should add type hints and follow a consistent docstring style.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the stack and the minimum stack.\"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        \"\"\"Push element x onto the stack.\n",
      "\n",
      "        Args:\n",
      "            x (int): The element to be pushed onto the stack.\n",
      "        \"\"\"\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        \"\"\"Remove the element on top of the stack.\"\"\"\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        \"\"\"Get the top element of the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The top element of the stack, or None if the stack is empty.\n",
      "        \"\"\"\n",
      "        return self.stack[-1] if self.stack else None\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        \"\"\"Retrieve the minimum element in the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The minimum element in the stack, or None if the stack is empty.\n",
      "        \"\"\"\n",
      "        return self.min_stack[-1] if self.min_stack else None\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Example usage:\n",
      "    min_stack = MinStack()\n",
      "    min_stack.push(-2)\n",
      "    min_stack.push(0)\n",
      "    min_stack.push(-3)\n",
      "    print(min_stack.getMin())  # Returns -3\n",
      "    min_stack.pop()\n",
      "    print(min_stack.top())     # Returns 0\n",
      "    print(min_stack.getMin())  # Returns -2\n",
      "```\n",
      "\n",
      "This implementation ensures that all operations (`push`, `pop`, `top`, and `getMin`) are performed in O(1) time complexity. It handles empty stack scenarios gracefully by returning `None` and includes clear documentation with type hints and docstrings.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: Code correctness, style, and best practices have been improved but still have room for improvement. Here are some suggestions:\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The current implementation is mostly correct and handles edge cases by returning `None` for empty stack scenarios. However, we can further improve the code by:\n",
      "1. Ensuring consistent method naming (e.g., `getMin` to `get_min` for consistency with `get_top_element`).\n",
      "2. Adding type hints and docstrings for better readability and maintainability.\n",
      "3. Ensuring that the implementation adheres to best practices by handling empty stack scenarios gracefully and providing clear documentation.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the stack and the minimum stack.\"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        \"\"\"Push element x onto the stack.\n",
      "\n",
      "        Args:\n",
      "            x (int): The element to be pushed onto the stack.\n",
      "        \"\"\"\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        \"\"\"Remove the element on top of the stack.\"\"\"\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def get_top_element(self) -> int:\n",
      "        \"\"\"Get the top element of the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The top element of the stack, or None if the stack is empty.\n",
      "        \"\"\"\n",
      "        return self.stack[-1] if self.stack else None\n",
      "\n",
      "    def get_min(self) -> int:\n",
      "        \"\"\"Retrieve the minimum element in the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The minimum element in the stack, or None if the stack is empty.\n",
      "        \"\"\"\n",
      "        return self.min_stack[-1] if self.min_stack else None\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Example usage:\n",
      "    min_stack = MinStack()\n",
      "    min_stack.push(-2)\n",
      "    min_stack.push(0)\n",
      "    min_stack.push(-3)\n",
      "    print(min_stack.get_min())  # Returns -3\n",
      "    min_stack.pop()\n",
      "    print(min_stack.get_top_element())     # Returns 0\n",
      "    print(min_stack.get_min())  # Returns -2\n",
      "```\n",
      "\n",
      "This implementation ensures that all operations (`push`, `pop`, `get_top_element`, and `get_min`) are performed in O(1) time complexity. It handles empty stack scenarios gracefully by returning `None` and includes clear documentation with type hints and docstrings. The method naming is consistent and follows best practices.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: PASS\n",
      "Feedback: No suggestions for improvement.\n",
      "=== EVALUATION END ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thoughts:\\nThe current implementation is mostly correct and handles edge cases by returning `None` for empty stack scenarios. However, we can further improve the code by:\\n1. Ensuring consistent method naming (e.g., `getMin` to `get_min` for consistency with `get_top_element`).\\n2. Adding type hints and docstrings for better readability and maintainability.\\n3. Ensuring that the implementation adheres to best practices by handling empty stack scenarios gracefully and providing clear documentation.\\n\\nResponse:\\n```python\\nclass MinStack:\\n    def __init__(self):\\n        \"\"\"Initialize the stack and the minimum stack.\"\"\"\\n        self.stack = []\\n        self.min_stack = []\\n\\n    def push(self, x: int) -> None:\\n        \"\"\"Push element x onto the stack.\\n\\n        Args:\\n            x (int): The element to be pushed onto the stack.\\n        \"\"\"\\n        self.stack.append(x)\\n        if not self.min_stack or x <= self.min_stack[-1]:\\n            self.min_stack.append(x)\\n\\n    def pop(self) -> None:\\n        \"\"\"Remove the element on top of the stack.\"\"\"\\n        if self.stack:\\n            x = self.stack.pop()\\n            if x == self.min_stack[-1]:\\n                self.min_stack.pop()\\n\\n    def get_top_element(self) -> int:\\n        \"\"\"Get the top element of the stack.\\n\\n        Returns:\\n            int: The top element of the stack, or None if the stack is empty.\\n        \"\"\"\\n        return self.stack[-1] if self.stack else None\\n\\n    def get_min(self) -> int:\\n        \"\"\"Retrieve the minimum element in the stack.\\n\\n        Returns:\\n            int: The minimum element in the stack, or None if the stack is empty.\\n        \"\"\"\\n        return self.min_stack[-1] if self.min_stack else None\\n\\nif __name__ == \"__main__\":\\n    # Example usage:\\n    min_stack = MinStack()\\n    min_stack.push(-2)\\n    min_stack.push(0)\\n    min_stack.push(-3)\\n    print(min_stack.get_min())  # Returns -3\\n    min_stack.pop()\\n    print(min_stack.get_top_element())     # Returns 0\\n    print(min_stack.get_min())  # Returns -2\\n```\\n\\nThis implementation ensures that all operations (`push`, `pop`, `get_top_element`, and `get_min`) are performed in O(1) time complexity. It handles empty stack scenarios gracefully by returning `None` and includes clear documentation with type hints and docstrings. The method naming is consistent and follows best practices.'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_workflow(task, EVALUATOR_PROMPT, GENERATOR_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt Chaining Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "\n",
    "class DialogueItem(BaseModel):\n",
    "    \"\"\"A single dialogue item.\"\"\"\n",
    "\n",
    "    speaker: Literal[\"Host (Jane)\", \"Guest\"]\n",
    "    text: str\n",
    "\n",
    "\n",
    "class Dialogue(BaseModel):\n",
    "    \"\"\"The dialogue between the host and guest.\"\"\"\n",
    "\n",
    "    scratchpad: str\n",
    "    name_of_guest: str\n",
    "    dialogue: List[DialogueItem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an experienced world-class podcast producer tasked with transforming the provided \n",
    "input text into an engaging and informative podcast.\n",
    "\n",
    "You are to follow a step by step methodical process to generate the final podcast which involves:\n",
    "1. Reading and extracting relevant information and snippets from the source document.\n",
    "2. Using the relevant information compiled in step 1, creating an outline document containing brainstormed ideas, summarized topics that should be covered, questions and how to guide the conversation \n",
    "3. Using the details from step 1 and 2 you then need to put together a script for the podcast.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "CLEAN_EXTRACT_DETAILS = \"\"\"The first step you need to perform is to extract details from the source document that are informative\n",
    "and listeners will find useful to understand the source document better.\n",
    "\n",
    "The input may be unstructured or messy, sourced from PDFs or web pages. \n",
    "\n",
    "Your goal is to extract the most interesting and insightful content for a compelling podcast discussion.\n",
    "\n",
    "Source Document: {source_doc}\n",
    "\"\"\"\n",
    "\n",
    "OUTLINE_PROMPT = \"\"\"The second step is to use the extracted information from the source document to write an outline and brainstorm ideas.\n",
    "\n",
    "The source document and extracted details are provided below:\n",
    "\n",
    "Extracted Details: {extracted_details}\n",
    "\n",
    "Source Document: {source_doc}\n",
    "\n",
    "Steps to follow when generating an outline and brainstorming ideas for the discussion in the podcast:\n",
    "\n",
    "1. Analyze the Input:\n",
    "   Carefully examine the extracted details in the text above, identifying key topics, points, and \n",
    "   interesting facts or anecdotes that could drive an engaging podcast conversation. \n",
    "   Disregard irrelevant information.\n",
    "\n",
    "2. Brainstorm Ideas:\n",
    "   Creatively brainstorm ways to present the key points engagingly. \n",
    "   \n",
    "   Consider:\n",
    "   - Analogies, storytelling techniques, or hypothetical scenarios to make content relatable\n",
    "   - Ways to make complex topics accessible to a general audience\n",
    "   - Thought-provoking questions to explore during the podcast\n",
    "   - Creative approaches to fill any gaps in the information\n",
    "   - Make sure that all important details extracted above are covered in the outline that you draft\n",
    "\"\"\"\n",
    "\n",
    "SCRIPT_PROMPT = \"\"\"The last step is to use the extracted details and the ideas brainstormed in the outline below to craft\n",
    "a script for the podcast.\n",
    "\n",
    "Extracted Details: {extracted_details}\n",
    "\n",
    "Using the outline provided here: {outline}\n",
    "\n",
    "Steps to follow when generating the script:\n",
    "\n",
    " 1. **Craft the Dialogue:**\n",
    "   Develop a natural, conversational flow between the host (Jane) and the guest speaker (the author or an expert on the topic).\n",
    "   In the `<scratchpad>`, creatively brainstorm ways to present the key points engagingly.\n",
    "   \n",
    "   Incorporate:\n",
    "   - The best ideas from your brainstorming session\n",
    "   - Clear explanations of complex topics\n",
    "   - An engaging and lively tone to captivate listeners\n",
    "   - A balance of information and entertainment\n",
    "\n",
    "   Rules for the dialogue:\n",
    "   - The host (Jane) always initiates the conversation and interviews the guest\n",
    "   - Include thoughtful questions from the host to guide the discussion\n",
    "   - Incorporate natural speech patterns, including occasional verbal fillers (e.g., \"Uhh\", \"Hmmm\", \"um,\" \"well,\" \"you know\")\n",
    "   - Allow for natural interruptions and back-and-forth between host and guest - this is very important to make the conversation feel authentic\n",
    "   - Ensure the guest's responses are substantiated by the input text, avoiding unsupported claims\n",
    "   - Maintain a PG-rated conversation appropriate for all audiences\n",
    "   - Avoid any marketing or self-promotional content from the guest\n",
    "   - The host concludes the conversation\n",
    "\n",
    "2. **Summarize Key Insights:**\n",
    "   Naturally weave a summary of key points into the closing part of the dialogue. This should feel like a casual conversation rather than a formal recap, reinforcing the main takeaways before signing off.\n",
    "\n",
    "3. **Maintain Authenticity:**\n",
    "   Throughout the script, strive for authenticity in the conversation. Include:\n",
    "   - Moments of genuine curiosity or surprise from the host\n",
    "   - Instances where the guest might briefly struggle to articulate a complex idea\n",
    "   - Light-hearted moments or humor when appropriate\n",
    "   - Brief personal anecdotes or examples that relate to the topic (within the bounds of the input text)\n",
    "\n",
    "4. **Consider Pacing and Structure:**\n",
    "   Ensure the dialogue has a natural ebb and flow:\n",
    "   - Start with a strong hook to grab the listener's attention\n",
    "   - Gradually build complexity as the conversation progresses\n",
    "   - Include brief \"breather\" moments for listeners to absorb complex information\n",
    "   - For complicated concepts, reasking similar questions framed from a different perspective is recommended\n",
    "   - End on a high note, perhaps with a thought-provoking question or a call-to-action for listeners\n",
    "\n",
    "IMPORTANT RULE: Each line of dialogue should be no more than 300 characters (e.g., can finish within 30 seconds)\n",
    "\n",
    "Remember: Always reply in valid JSON format, without code blocks. Begin directly with the JSON output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-07 22:42:44--  https://arxiv.org/pdf/2406.04692\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.131.42, 151.101.195.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1157463 (1.1M) [application/pdf]\n",
      "Saving to: ‘2406.04692’\n",
      "\n",
      "2406.04692          100%[===================>]   1.10M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-01-07 22:42:45 (25.5 MB/s) - ‘2406.04692’ saved [1157463/1157463]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://arxiv.org/pdf/2406.04692\n",
    "!mv 2406.04692 MoA.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mixture-of-Agents Enhances Large Language Model\\nCapabilities\\nJunlin Wang\\nDuke University\\nTogether AI\\njunlin.wang2@duke.edu\\nJue Wang\\nTogether AI\\njue@together.ai\\nBen Athiwaratkun\\nTogether AI\\nben@together.ai\\nCe Zhang\\nUniversity of Chicago\\nTogether AI\\ncez@uchicago.edu\\nJames Zou\\nStanford University\\nTogether AI\\njamesz@stanford.edu\\nAbstract\\nRecent advances in large language models (LLMs) demonstrate substantial capa-\\nbilities in natural language understanding and generation tasks. With the growing\\nnumber of LLMs, how to harness the collective expertise of multiple LLMs is an\\nexciting open direction. Toward this goal, we propose a new approach that lever-\\nages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA)\\nmethodology. In our approach, we construct a layered MoA architecture wherein\\neach layer comprises multiple LLM agents. Each agent takes all the outputs from\\nagents in the previous layer as auxiliary information in generating its response.\\nMoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and\\nFLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source\\nLLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of\\n65.1% compared to 57.5% by GPT-4 Omni.1\\n1 Introduction\\nLarge language models (LLMs) (Zhang et al., 2022a; Chowdhery et al., 2022; Touvron et al., 2023a;\\nTeam et al., 2023; Brown et al., 2020; OpenAI, 2023) have significantly advanced the field of natural\\nlanguage understanding and generation in recent years. These models are pretrained on vast amounts\\nof data and subsequently aligned with human preferences to generate helpful and coherent outputs\\n(Ouyang et al., 2022). However, despite the plethora of LLMs and their impressive achievements,\\nthey still face inherent constraints on model size and training data. Further scaling up these models is\\nexceptionally costly, often requiring extensive retraining on several trillion tokens.\\nAt the same time, different LLMs possess unique strengths and specialize in various tasks aspects.\\nFor instance, some models excel at complex instruction following (Xu et al., 2023a) while others may\\nbe better suited for code generation (Roziere et al., 2023; Guo et al., 2024). This diversity in skill sets\\namong different LLMs presents an intriguing question: Can we harness the collective expertise of\\nmultiple LLMs to create a more capable and robust model?\\nOur answer to this question isYes. We identify an inherent phenomenon we term thecollaborativeness\\nof LLMs — wherein an LLM tends to generate better responses when presented with outputs\\nfrom other models, even if these other models are less capable by itself. Figure 1 showcases\\nthe LC win rate on the AlpacaEval 2.0 benchmark (Dubois et al., 2024) for 6 popular LLMs.\\n1Our code can be found in: https://github.com/togethercomputer/moa.\\nPreprint. Under review.\\narXiv:2406.04692v1  [cs.CL]  7 Jun 2024\\n\\n[Prompt]\\nA1,1\\nA1,2\\nA1,3\\n[Intermediate Output]\\nA2,1\\nA2,2\\nA2,3\\n[Intermediate Output]\\nA4,1\\nLayer 1\\nA3,1\\nA3,2\\nA3,3\\n[Intermediate Output]\\nLayer 2 Layer 3 Layer 4\\n[Final Output]\\nconcatenate\\nAgent:\\nToken:\\nAi,j\\nFigure 2: Illustration of the Mixture-of-Agents Structure. This example showcases 4 MoA layers\\nwith 3 agents in each layer. The agents here can share the same model.\\nFigure 1: AlpacaEval 2.0 LC win rates im-\\nprove when provided with responses from\\nother models.\\nWhen these models are provided with answers gen-\\nerated independently by these models, their LC win\\nrates significantly improve. This indicates that the\\ncollaborativeness phenomenon is widespread among\\nLLMs. Remarkably, this improvement occurs even\\nwhen the auxiliary responses provided by the other\\nmodels are of lower quality than what an individual\\nLLM could generate independently.\\nBased on this finding, this paper introduces a Mixture-\\nof-Agents (MoA) methodology that leverages multi-\\nple LLMs to iteratively enhance the generation qual-\\nity. The structure of MoA is illustrated in Figure 2.\\nInitially, LLMs in the first layer, denoted as agents\\nA1,1, ...A1,n independently generate responses to a\\ngiven prompt. These responses are then presented\\nto agents in the next layer A2,1, ...A2,n (which may reuse a model from the first layer) for further\\nrefinement. This iterative refinement process continues for several cycles until obtaining a more\\nrobust and comprehensive response.\\nTo ensure effective collaboration among models and improve overall response quality, careful\\nselection of LLMs for each MoA layer is crucial. This selection process is guided by two primary\\ncriteria: (a) Performance Metrics: The average win rate of models in layer i plays a significant role in\\ndetermining their suitability for inclusion in layer i + 1. Therefore, selecting models based on their\\ndemonstrated performance metrics ensures higher-quality outputs. (b) Diversity Considerations: The\\ndiversity of model outputs is also crucial. Responses generated by heterogeneous models contribute\\nsignificantly more than those produced by the same model as we show later in section 3.3. By\\nleveraging these criteria — performance and diversity — MoA aims to mitigate individual model\\ndeficiencies and enhance overall response quality through collaborative synthesis.\\nWe conduct comprehensive evaluations using AlpacaEval 2.0, MT-Bench (Zheng et al., 2023), FLASK\\n(Ye et al., 2023) benchmarks for assessing the response quality across various dimensions. The results\\ndemonstrate substantial improvements with our proposed method, achieving a new SOTA win rate of\\n65.8% on AlpacaEval 2.0 compared to the previous best of 57.5% achieved by GPT-4 Omni.\\nThe contributions of this work are summarized as follows: (1) Novel framework: we propose\\na Mixture-of-Agents framework designed to leverage the strengths of multiple LLMs, thereby\\nimproving their reasoning and language generation capabilities. (2) Finding of collaborativeness\\nof language models: we highlight the inherit collaborativeness among LLMs, where models tend\\nto generate better quality responses when they have access to outputs from other models, even if\\nthose outputs are of lower quality. (3) State-of-the-art LLM performance: we conducted extensive\\nexperiments using multiple highly-competitive benchmarks such as AlpacaEval 2.0, MT-Bench, and\\nFLASK; our MoA framework achieves state-of-the-art performance on these benchmarks.\\n2\\n\\n2 Mixture-of-Agents Methodology\\nIn this section, we present our proposed methodology for leveraging multiple models to achieve\\nboosted performance. We begin by demonstrating that LLMs possess collaborativeness and thus\\ncan improve their responses based on the outputs of other models. Following this, we introduce the\\nMixture-of-Agents methodology and discuss its design implications.\\n2.1 Collaborativeness of LLMs\\nWe begin by demonstrating the collaborativeness of LLMs, specifically their ability to generate higher\\nquality responses when they can reference outputs from other models. As we have shown in the\\nintroduction and Figure 1, many of today’s available LLMs exhibit this collaborative capability.\\nAn important pathway to extract maximum benefits from collaboration of multiple LLMs is to\\ncharacterize how different models are good at in various aspects of collaboration. During the\\ncollaboration process, we can categorize LLMs into two distinct roles:\\nProposers excel at generating useful reference responses for use by other models. While a good\\nproposer may not necessarily produce responses with high scores by itself, it should offer more\\ncontext and diverse perspectives, ultimately contributing to better final responses when used by an\\naggregator.\\nAggregators are models proficient in synthesizing responses from other models into a single, high-\\nquality output. An effective aggregator should maintain or enhance output quality even when\\nintegrating inputs that are of lesser quality than its own.\\nSection 3.3 empirically validate the roles of aggregators and proposers. Specifically, we show that\\nmany LLMs possess capabilities both as aggregators and proposers, while certain models displayed\\nspecialized proficiencies in distinct roles. GPT-4o, Qwen1.5, LLaMA-3 emerged as a versatile model\\neffective in both assisting and aggregating tasks. In contrast, WizardLM demonstrated excellent\\nperformance as an proposer model but struggled to maintain its effectiveness in aggregating responses\\nfrom other models.\\nGiven that an aggregator can generate higher-quality responses by building upon outputs from\\nother models, we propose further enhancing this collaborative potential by introducing additional\\naggregators. One intuitive idea is to replicate the exercise with multiple aggregators — initially\\nusing several to aggregate better answers and then re-aggregating these aggregated answers. By\\nincorporating more aggregators into the process, we can iteratively synthesize and refine the responses,\\nleveraging the strengths of multiple models to produce superior outcomes. This leads to the design of\\nour proposed Mixture-of-Agents.\\n2.2 Mixture-of-Agents\\nThe structure of MoA is illustrated in Figure 2. It has l layers and each layer-i consists of n LLMs,\\ndenoted by Ai,1, Ai,2, ..., Ai,n. It is important to note that LLMs can be reused either within the\\nsame layer or across different layers. When many LLMs in a layer are identical, this configuration\\nleads to a special structure that corresponds to a model generating multiple possibly different outputs\\n(due to the stochasticity of temperature sampling). We refer to this setting as single-proposer, where\\nonly a sparse subset of models are activated.\\nHere, each LLM Ai,j processes an input text and generates its continuation. Our method does not\\nrequire any fine-tuning and only utilizes the interface of prompting and generation of LLMs. Formally,\\ngiven an input prompt x1, the output of i-th MoA layer yi can be expressed as follows:\\nyi = ⊕n\\nj=1[Ai,j(xi)] + x1, xi+1 = yi (1)\\nwhere + here means concatenation of texts; ⊕ means application of the Aggregate-and-Synthesize\\nprompt shown in Table 1 to these model outputs.\\nIn practice, we do not need to concatenate prompt and all model responses so only one LLM is needed\\nto be used in the last layer. Therefore, we use the output of an LLM from the l-th layer (Al,1(xl)) as\\nthe final output and evaluate the metrics based on it.\\n3\\n\\nTable 1: Aggregate-and-Synthesize Prompt to integrate responses from other models.\\nYou have been provided with a set of responses from various open-source models to the latest user query. Your\\ntask is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the\\ninformation provided in these responses, recognizing that some of it may be biased or incorrect. Your response\\nshould not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply\\nto the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of\\naccuracy and reliability.\\nResponses from models:\\n1. [Model Response from Ai,1]\\n2. [Model Response from Ai,2]\\n...\\nn. [Model Response from Ai,n]\\n2.3 Analogy to Mixture-of-Experts\\nMixture-of-Experts (MoE) (Shazeer et al., 2017) is a prominent and well-established technique\\nin machine learning where multiple expert networks specialize in different skill sets. The MoE\\napproach has shown significant success across various applications due to its ability to leverage\\ndiverse model capabilities for complex problem-solving tasks. Our MoA method draws inspiration\\nfrom this methodology.\\nA typical MoE design consists of a stack of layers known as MoE layers. Each layer comprises a\\nset of n expert networks alongside a gating network and includes residual connections for improved\\ngradient flow. Formally, for layer i, this design can be expressed as follows:\\nyi =\\nnX\\nj=1\\nGi,j(xi)Ei,j(xi) +xi (2)\\nwhere Gi,j represents the output from the gating network corresponding to expert j, and Ei,j denotes\\nthe function computed by expert network j. The leverage of multiple experts allows the model to\\nlearn different skill sets and focus on various aspects of the task at hand.\\nFrom a high-level perspective, our proposed MoA framework extends the MoE concept to the model\\nlevel by operating at the model level rather than at the activation level. Specifically, our MoA approach\\nleverages LLMs and operates entirely through the prompt interface rather than requiring modifications\\nto internal activations or weights. This means that instead of having specialized sub-networks within\\na single model like in MoE, we utilize multiple full-fledged LLMs across different layers. Note that\\nin our approach, we consolidate the roles of the gating network and expert networks using a LLM, as\\nthe intrinsic capacity of LLMs allows them to effectively regularize inputs by interpreting prompts\\nand generating coherent outputs without needing external mechanisms for coordination.\\nMoreover, since this method relies solely on prompting capabilities inherent within off-the-shelf\\nmodels: (1) It eliminates computational overhead associated with fine-tuning; (2) It provides flexibility\\nand scalability: our method can be applied to the latest LLMs regardless of their size or architecture.\\n3 Evaluation\\nThis section presents a comprehensive evaluation of our proposed MoA. Our findings show that:\\n1. We achieve significant improvements on AlpacaEval 2.0, MT-Bench, and FLASK bench-\\nmarks. Notably, with open-source models only, our approach outperforms GPT-4o on\\nAlpacaEval 2.0 and FLASK.\\n2. We conduct extensive experiments to provide better understandings of the internal mecha-\\nnism of MoA.\\n3. Through a detailed budget analysis, several implementations of MoA can deliver perfor-\\nmance comparable to GPT-4 Turbo while being 2× more cost-effective.\\n4\\n\\nTable 2: Results on AlpacaEval 2.0 and MT-Bench. For AlpacaEval 2.0, MoA and MoA-Lite\\ncorrespond to the 6 proposer with 3 layers and with 2 layer respectively. MoA w/ GPT-4o corresponds\\nto using GPT-4o as the final aggregator in MoA. We ran our experiments three times and reported the\\naverage scores along with the standard deviation. † denotes our replication of the AlpacaEval results.\\nWe ran all the MT-Bench scores ourselves to get turn-based scores.\\n(a) AlpacaEval 2.0\\nModel LC win. win.\\nMoA w/ GPT-4o 65.7±0.7% 78.7±0.2%\\nMoA 65.1±0.6% 59.8±0.3%\\nMoA-Lite 59.3±0.2% 57.0±0.7%\\nGPT-4 Omni (05/13) 57.5% 51.3%\\nGPT-4 Turbo (04/09) 55.0% 46.1%\\nWizardLM 8x22B† 51.3% 62.3%\\nGPT-4 Preview (11/06) 50.0% 50.0%\\nQwen1.5 110B Chat 43.9% 33.8%\\nQwen1.5 72B Chat 36.6% 26.5%\\nGPT-4 (03/14) 35.3% 22.1%\\nLlama 3 70B Instruct 34.4% 33.2%\\nMixtral 8x22B v0.1 30.9% 22.2%\\n(b) MT-Bench.\\nModel Avg. 1st turn 2nd turn\\nMoA w/ GPT-4o 9.40±0.06 9.49 9.31\\nGPT-4 Turbo (04/09) 9.31 9.35 9.28\\nMoA 9.25±0.10 9.44 9.07\\nGPT-4 Preview (11/06) 9.20 9.38 9.03\\nGPT-4 Omni (05/13) 9.19 9.31 9.07\\nMoA-Lite 9.18±0.09 9.38 8.99\\nQwen1.5 110B Chat 8.96 9.23 8.63\\nLlama 3 70B Instruct 8.94 9.2 8.68\\nMixtral 8x22B v0.1 8.78 9.11 8.44\\nWizardLM 8x22B 8.78 8.96 8.61\\nQwen1.5 72B Chat 8.44 8.55 8.34\\nGPT-4 (06/13) 8.84 9.08 8.61\\n3.1 Setup\\nBenchmarks We mainly evaluate models on AlpacaEval 2.0 (Dubois et al., 2024), a leading\\nbenchmark for assessing the alignment of LLMs with human preferences. It contains 805 instructions\\nrepresentative of real use cases. Each model’s response is directly compared against that of the GPT-4\\n(gpt-4-1106-preview), with a GPT-4-based evaluator determining the likelihood of preferring the\\nevaluated model’s response. To ensure fairness, the evaluation employs length-controlled (LC) win\\nrates, effectively neutralizing length bias.2\\nAdditionally, we also evaluate on MT-Bench (Zheng et al., 2023) and FLASK (Ye et al., 2023).\\nMT-Bench uses GPT-4 to grade and give a score to model’s answer. FLASK, on the other hand, offers\\na more granular evaluation with 12 skill-specific scores.\\nModels In our study, we constructed our default MoA by using only open-source models to achieve\\ncompetitive performance. The models included are: Qwen1.5-110B-Chat (Bai et al., 2023), Qwen1.5-\\n72B-Chat, WizardLM-8x22B (Xu et al., 2023a), LLaMA-3-70B-Instruct (Touvron et al., 2023b),\\nMixtral-8x22B-v0.1 (Jiang et al., 2024), dbrx-instruct (The Mosaic Research Team, 2024). We\\nconstruct 3 MoA layers and use the same set of models in each MoA layer. We use Qwen1.5-110B-\\nChat as the aggregator in the last layer. We also developed a variant called MoA w/ GPT-4o, which\\nprioritizes high-quality outputs by using GPT-4o as the aggregator in the final MoA layer. Another\\nvariant, MoA-Lite, emphasizes cost-effectiveness. It uses the same set of models as proposers but\\nincludes only 2 MoA layers and employs Qwen1.5-72B-Chat as the aggregator. This makes it more\\ncost-effective than GPT-4o while achieving a1.8% improvement in quality on AlpacaEval 2.0. We\\nensure strict adherence to the licensing terms of all models utilized in this research. For open-source\\nmodels, all inferences were ran through Together Inference Endpoint.3\\n3.2 Benchmark Results\\nIn this subsection, we present our evaluation results on three standard benchmarks: AlpacaEval 2.0,\\nMT-Bench, and FLASK. These benchmarks were chosen to comprehensively assess the performance\\nof our approach and compare with the state-of-the-art LLMs.\\n2This metric tracks closely with human preferences, achieving a Spearman correlation of 0.98 with actual\\nhuman evaluations (Dubois et al., 2024).\\n3https://api.together.ai/playground/chat\\n5\\n\\nAlpacaEval 2.0 We conducted comparisons against leading models such as GPT-4 and other\\nstate-of-the-art open-source models. The detailed results are presented in Table 2a where our MoA\\nmethodology achieved top positions on the AlpacaEval 2.0 leaderboard, demonstrating a remarkable\\n8.2% absolute improvement over the previous top model, GPT-4o. Moreover, it is particularly\\nnoteworthy that our model outperformed GPT-4o using solely open-source models, achieving a\\nmargin of 7.6% absolute improvement from 57.5% (GPT-4o) to 65.1% (MoA). Our MoA-Lite setup\\nuses less layers and being more cost-effective. Even with this lighter approach, we still outperform the\\nbest model by 1.8%, improving from 57.5% (GPT-4o) to 59.3% (MoA-Lite). This further highlights\\nthe effectiveness of our method in leveraging open-source models capabilities with varying compute\\nbudget to their fullest potential.\\nrobustness\\ncorrectness\\nefficiency\\nfactuality\\ncommonsense\\ncomprehension\\ninsightfulness\\ncompleteness\\nmetacognition\\nreadability\\nconciseness\\nharmlessness\\n3 3.5 4 4.5 5\\nGPT-4 Omni (05/13)\\nGPT-3.5-turbo-0125\\nQwen1.5-110B-Chat\\nMoA\\nFigure 3: Results on FLASK where we use the 6 pro-\\nposer MoA setup and Qwen1.5-110B-Chat is the aggre-\\ngator.\\nMT-Bench Though improvements over\\nindividual models on the MT-Bench are rel-\\natively incremental, this is understandable\\ngiven that current models already perform\\nexceptionally well on this benchmark, as\\na single model alone can achieve scores\\ngreater than 9 out of 10. Despite the\\nmarginal enhancements, our approach still\\nsecures the top position on the leaderboard.\\nThis demonstrates that even with already\\nhighly optimized benchmarks, our method\\ncan push the boundaries further, maintain-\\ning the leadership.\\nFLASK FLASK provides fine-grained\\nevaluation of models. Among those met-\\nrics, MoA excels in several key aspects.\\nSpecifically, our methodology shows signif-\\nicant improvement in robustness, correct-\\nness, efficiency, factuality, commonsense,\\ninsightfulness, completeness, compared to\\nthe single model score of the aggregator,\\nQwen-110B-Chat. Additionally, MoA also\\noutperforms GPT-4 Omni in terms of correctness, factuality, insightfulness, completeness, and\\nmetacognition. One metric where MoA did not do as well was conciseness; the model produced\\noutputs that were marginally more verbose.\\n3.3 What Makes Mixture-of-Agents Work Well?\\nIn this subsection, we conduct experiments that provide us better understandings of the internal\\nmechanism of Mixture-of-Agents. We summarize key insights below.\\nMixture-of-Agents significantly outperforms LLM rankers. First, we compare Mixture-of-\\nAgents with an LLM-based ranker which uses the aggregator model to select one of the answers that\\nare generated by the proposers, instead of generating a new output. The results are shown in Figure 4,\\nwhere we can observe that the MoA approach significantly outperforms an LLM-ranker baseline. The\\nfact that MoA outperforms the ranking approach suggests that the aggregator does not simply select\\none of the generated answers by the proposers, but potentially performs sophisticated aggregation\\nover all proposed generations.\\nMoA tends to incorporate the best proposed answers. We also compare the aggregator’s response\\nwith the proposers’ responses via similarity scores such as BLEU (Papineni et al., 2002) which reflects\\nn-gram overlaps. Within each sample, givenn proposed answers by the proposers, we calculate the the\\nSpearman’s rank correlation coefficient between n similar scores and n preference scores determined\\nby the GPT-4 based evaluator. The results in Figure 4 indeed confirms a positive correlation between\\nthe win rate and the BLEU score. We also provide results with Levenshtein similarity (RapidFuzz,\\n2023) or TF-IDF as opposed to BLEU scores in Appendix A. where both alternative approaches for\\ntextual similarities also yield positive correlation with the preference scores.\\n6\\n\\nLayer 1 Layer 2 Layer 3 Layer 4\\n \\n20\\n30\\n40\\n50\\n60\\n70LC win rate\\nGPT-4 Preview\\nGPT-4 Omni\\nGPT-4o\\nQwen1.5-110B-Chat\\nQwen1.5-72B-Chat\\nWizard 8x22b\\nMixtral-8x22B-Instruct-v0.1\\nLlama-3-70B-Instruct\\ndbrx-instruct\\nLLM-Ranker\\n0.00 0.05 0.10 0.15 0.20 0.25 0.30\\nSpearman correlation coefficient\\nQWen1.5-110B\\nQWen1.5-72B\\nWizardLM\\nLlama-3-70B\\nMixtral-8x22B\\ndbrx-instruct\\nAggregator\\nAggregation\\n1st aggregation\\n2nd aggregation\\n3rd aggregation\\nFigure 4: (a) LC win rate on AlpacaEval 2.0 with different aggregators in the 6-model Mixture-of-\\nAgents setup. All the curves use the same 6 proposer agents; they only differ in the choice of the final\\naggregator. The LLM ranker uses Qwen1.5-110B-Chat model with a prompt format in Appendix\\nTable 5. The GPT-4o model is only used to aggregate the output for the purpose of evaluation and\\ndoes not participate as a proposer towards the next layer. (b) Spearman correlation between BLEU\\nscores (calculated using 3-gram, 4-gram, and 5-gram metrics) and win rate of the proposed outputs.\\nTable 3: Effects of the number of proposer models\\non AlpacaEval 2.0. We denote n as either the\\nnumber of agents in an MoA layer or the number\\nof proposed outputs in the single-proposer setting.\\nWe use Qwen1.5-110B-Chat as the aggregator\\nand use 2 MoA layers for all settings in this table.\\nSetting Multiple-Proposer Single-Proposer\\nn = 6 61.3% 56.7%\\nn = 3 58.0% 56.1%\\nn = 2 58.8% 54.5%\\nn = 1 47.8% 47.8%\\nTable 4: Impact of different models serving as\\nproposers vs aggregators. When evaluating differ-\\nent aggregators, all six models serve as proposers;\\nwhen evaluating proposers, Qwen1.5-110B-Chat\\nserves as the aggregator. We use 2 MoA layers in\\nthis table.\\nModel As aggregator As proposer\\nQwen1.5-110B-Chat 61.3% 56.7%\\nQwen1.5-72B-Chat 59.3% 53.3%\\nLLaMA-3-70b-Instruct 45.0% 60.6%\\nWizardLM 8x22B 52.9% 63.8%\\nMixtral-8x22B-Instruct 48.4% 54.8%\\ndbrx-instruct 41.5% 55.1%\\nEffect of model diversity and the number of proposers. We analyze how the number of proposals\\naffect the final output quality by varying n, the number of proposers in each layer. We show the\\nresults in Table 3 where we find that scores increases monotonically with n, reflecting the benefits\\nof having more auxiliary information. In addition, we also quantify the impact of using a diverse\\nset of LLMs as proposers. For each n, we compare two settings: “single-proposer” where the n\\nresponses are generated by the same LLM with a temperature of 0.7; and “multiple-proposer” where\\neach response is generated by a different LLMs. Overall, using multiple different LLMs consistently\\nyielded better results. Both results suggest that having a larger number of diverse LLM agents in each\\nMoA layer can improve performance. Further scaling the width of MoA is a promising direction of\\nfuture investigation.\\nSpecialization of models in the Mixture-of-Agent ecosystem. We also conducted experiments\\nto determine which models excel in specific roles. Specifically, Table 4 shows that GPT-4o, Qwen,\\nLLaMA-3 emerged as a versatile model effective in both assisting and aggregating tasks. In contrast,\\nWizardLM demonstrated excellent performance as an proposer model but struggled to maintain its\\neffectiveness in aggregating responses from other models.\\n3.4 Budget and Token Analysis\\nTo understand the relationship between budget, token usage, and LC win rates, we conducted a budget\\nand token analysis. Figure 5a and Figure 5b illustrate these relationships.\\n7\\n\\n0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035\\nCost\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Score\\nGPT-4o\\nGPT-4-turbo\\nMoA-Lite\\nMoA\\nmodel type\\nMulti Proposer\\nSingle Proposer\\nlayer\\n1\\n2\\n3\\n(a) LC win rate vs. cost\\n50 100 150 200 250 300 350\\ntflops\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Score\\nMoA\\nMoA-Lite\\nGPT-4o\\nGPT-4-turbo\\nmodel type\\nMulti Proposer\\nSingle Proposer\\nlayer\\n1\\n2\\n3 (b) LC win rate vs. tflops\\nFigure 5: (a) Performance trade-off versus cost. (b) Performance trade-off versus the number of tera\\nfloating operations (tflops), which we use as a proxy for latency. Note that we calculate the sum\\nover layers of the max number of tflops among proposers in each MoA layer as multiple proposers\\ncan run in parallel. Our plots illustrate a Pareto frontier where we can choose a model progressively\\nhigher score with the lowest cost for such level of performance. We show that the Mixture-of-Agents\\napproach lie on this Pareto front, as opposed to GPT-4 Turbo and GPT-4o which are not cost optimal\\nand is more expensive compared to MoA approaches of the same LC win rate. Single Proposer: uses\\nthe same model to generate multiple responses in each MoA layer; Multi Proposer: uses different\\nmodels in each MoA layer. The actual tflops of GPT-4 is unknown, so we use the rumored size from\\nthe community of an 8x220B architecture.\\nCost Effectiveness In Figure 5a, we plot the LC win rate against the average inference cost for\\neach instance in the AplacaEval 2.0 benchmark. The cost is calculated based on pricing information\\navailable from API provider websites.4 This helps identify cost-effective models that achieve high\\nperformance without incurring excessive expenses. The chart reveals a Pareto front where certain\\nmodels strike an optimal balance between cost and performance. Models closer to this Pareto front\\nare more desirable as they provide better monetary value by delivering high LC win rates at lower\\ncosts. Specifically, if we prioritize the quality, MoA is the best configuration. However, if we want to\\nstrike a good balance between quality and cost, MoA-Lite can match GPT-4o’s cost while achieving\\nhigher level of quality. Notably, it outperforms GPT-4 Turbo by approximately4% while being more\\nthan twice as cost-effective.\\nTflops Consumption Figure 5b depicts the relationship between LC win rate and the number of\\ntflops. Here we use the number of tflops as a proxy for latency since latency can vary depending on\\nthe inference systems. This analysis is crucial for understanding how different models manage their\\nbudgets while maintaining or improving performance levels. Similar to the cost efficiency analysis, a\\nPareto front can be observed here as well. Models on this front effectively utilize their computational\\nresource to maximize their LC win rate.\\n4 Related Work\\n4.1 LLM Reasoning\\nIn order to improve generation quality of LLMs, recent researches have experienced great progresses\\nin optimizing LLMs to various downstream tasks through prompt engineering. Chain of Thought\\n(CoT) (Wei et al., 2022; Kojima et al., 2022) prompting techniques represent a linear problem-\\nsolving approach where each step builds upon the previous one. Fu et al. (2022) applied CoT to\\nmulti-step reasoning tasks. To automate CoT prompting, Auto-CoT (Zhang et al., 2022b) constructs\\ndemonstrations by sampling diverse questions and generating reasoning chains. Active-Prompt (Diao\\n4For open-source models, we calculate the price using data from https://api.together.ai/models;\\nfor OpenAI models, we use pricing details from https://openai.com/api/pricing/. Pricing data was\\nretrieved as of May 22, 2024.\\n8\\n\\net al., 2023) focuses on selecting the most uncertain questions for task-specific annotations. PS\\nPrompt (Wang et al., 2023) decomposes tasks into subtasks. Tree-of-Thought (ToT) (Yao et al., 2023a)\\nexpands on the reasoning process by considering multiple paths of reasoning and self-evaluating\\nchoices. Effective Graph-of-Thought (Yao et al., 2023b) frames thoughts as graphs. Natural Program\\nprompting (Ling et al., 2023) is proposed for better solving deductive reasoning tasks. And re-reading\\nprompt (Xu et al., 2023b) revisits question information embedded within input prompts.\\n4.2 Model Ensemble\\nA straightforward solution to leverage the strengths of multiple models is reranking outputs from\\ndifferent models. For instance, Jiang et al. (2023) introduce PAIR RANKER , which performs pairwise\\ncomparisons on candidate outputs to select the best one, showing improvements on a self-constructed\\ninstruction dataset. To address the substantial computational costs associated with multi-LLM\\ninference, other studies have explored training a router that predicts the best-performing model\\nfrom a fixed set of LLMs for a given input (Wang et al., 2024a; Shnitzer et al., 2024; Lu et al.,\\n2023). Additionally, FrugalGPT (Chen et al., 2023b) proposed reducing the cost of using LLMs\\nby employing different models in a cascading manner. In order to better leverage the responses of\\nmultiple models, Jiang et al. (2023) trained a GENFUSER , a model that was trained to generate an\\nimproved response to capitalize on the strengths of multiple candidates. Huang et al. (2024) proposed\\nto fuse the outputs of different models by averaging their output probability distributions.\\nAnother line of work is multi-agent collaboration. Several studies explore using multiple large\\nlanguage models as agents that collectively discuss and reason through given problems interactively.\\nDu et al. (2023) establishes a mechanism for symmetric discussions among agents. Around the same\\ntime, MAD (Liang et al., 2023) introduces an asymmetric mechanism design, with different roles, i.e.,\\ndebater and judge. Other similar works include (Chan et al., 2023). Moreover, ReConcile (Chen et al.,\\n2023a) exemplifies an asymmetric discussion involving weighted voting. To understand discussion\\nmore deeply, Zhang et al. (2023) aim to explain such collaboration mechanism in a social psychology\\nview. Wang et al. (2024b) systematically compared multi-agent approaches and found a single agent\\nwith a strong prompt including detailed demonstrations can achieve comparable response quality to\\nmulti-agent approaches.\\n5 Conclusion\\nThis paper introduces a Mixture-of-Agents approach aimed at leveraging the capabilities of multiple\\nLLMs via successive stages for iterative collaboration. Our method harnesses the collective strengths\\nof agents in the Mixture-of-Agents family, and can significantly improve upon the output quality of\\neach individual model. Empirical evaluations conducted on AlpacaEval 2.0, MT-Bench, and FLASK\\ndemonstrated substantial improvements in response quality, with our approach achieving the LC win\\nrate up to 65%. These findings validate our hypothesis that integrating diverse perspectives from\\nvarious models can lead to superior performance compared to relying on a single model alone. In\\naddition, we provide insights into improving the design of MoA; systematic optimization of MoA\\narchitecture is an interesting direction for future work.\\nLimitations. Our proposed method requires iterative aggregation of model responses, which means\\nthe model cannot decide the first token until the last MoA layer is reached. This potentially results\\nin a high Time to First Token (TTFT), which can negatively impact user experience. To mitigate\\nthis issue, we can limit the number of MoA layers, as the first response aggregation has the most\\nsignificant boost on generation quality. Future work could explore chunk-wise aggregation instead of\\naggregating entire responses at once, which can reduce TTFT while maintaining response quality.\\nBroader Impact. This study holds the potential to enhance the effectiveness of LLM-driven chat\\nassistants, thereby making AI more accessible. Moreover, since the intermediate outputs that are\\nexpressed in natural language, MoA presented improves the interpretability of models. This enhanced\\ninterpretability facilitates better alignment with human reasoning.\\n9\\n\\nReferences\\nBai, J., Bai, S., Chu, Y ., Cui, Z., Dang, K., Deng, X., Fan, Y ., Ge, W., Han, Y ., Huang, F., et al. Qwen\\ntechnical report. arXiv preprint arXiv:2309.16609, 2023.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\\nP., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural\\ninformation processing systems, 33:1877–1901, 2020.\\nChan, C.-M., Chen, W., Su, Y ., Yu, J., Xue, W., Zhang, S., Fu, J., and Liu, Z. Chateval: Towards\\nbetter llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.\\nChen, J. C.-Y ., Saha, S., and Bansal, M. Reconcile: Round-table conference improves reasoning via\\nconsensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023a.\\nChen, L., Zaharia, M., and Zou, J. Frugalgpt: How to use large language models while reducing cost\\nand improving performance. arXiv preprint arXiv:2305.05176, 2023b.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,\\nH. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv\\npreprint arXiv:2204.02311, 2022.\\nDiao, S., Wang, P., Lin, Y ., and Zhang, T. Active prompting with chain-of-thought for large language\\nmodels. arXiv preprint arXiv:2302.12246, 2023.\\nDu, Y ., Li, S., Torralba, A., Tenenbaum, J. B., and Mordatch, I. Improving factuality and reasoning\\nin language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\\nDubois, Y ., Galambosi, B., Liang, P., and Hashimoto, T. B. Length-controlled alpacaeval: A simple\\nway to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\\nFu, Y ., Peng, H., Sabharwal, A., Clark, P., and Khot, T. Complexity-based prompting for multi-step\\nreasoning. arXiv preprint arXiv:2210.00720, 2022.\\nGuo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y ., Li, Y ., et al.\\nDeepseek-coder: When the large language model meets programming–the rise of code intelligence.\\narXiv preprint arXiv:2401.14196, 2024.\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J.\\nMeasuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\\n2021.\\nHuang, Y ., Feng, X., Li, B., Xiang, Y ., Wang, H., Qin, B., and Liu, T. Enabling ensemble learn-\\ning for heterogeneous large language models with deep parallel collaboration. arXiv preprint\\narXiv:2404.12715, 2024.\\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S.,\\nde Las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R.,\\nSaulnier, L., Lachaux, M., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T.,\\nLavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts.CoRR, abs/2401.04088, 2024.\\ndoi: 10.48550/ARXIV .2401.04088. URLhttps://doi.org/10.48550/arXiv.2401.04088.\\nJiang, D., Ren, X., and Lin, B. Y . LLM-blender: Ensembling large language models with pairwise\\nranking and generative fusion. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.),Proceedings\\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), pp. 14165–14178, Toronto, Canada, July 2023. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2023.acl-long.792. URL https://aclanthology.org/2023.acl-long.\\n792.\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa, Y . Large language models are zero-shot\\nreasoners. Advances in neural information processing systems, 35:22199–22213, 2022.\\nLiang, T., He, Z., Jiao, W., Wang, X., Wang, Y ., Wang, R., Yang, Y ., Tu, Z., and Shi, S. Encour-\\naging divergent thinking in large language models through multi-agent debate. arXiv preprint\\narXiv:2305.19118, 2023.\\n10\\n\\nLing, Z., Fang, Y ., Li, X., Huang, Z., Lee, M., Memisevic, R., and Su, H. Deductive verification of\\nchain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.\\nLu, K., Yuan, H., Lin, R., Lin, J., Yuan, Z., Zhou, C., and Zhou, J. Routing to the expert: Efficient\\nreward-guided ensemble of large language models, 2023.\\nOpenAI. Gpt-4 technical report, 2023.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,\\nSlama, K., Ray, A., et al. Training language models to follow instructions with human feedback.\\nAdvances in neural information processing systems, 35:27730–27744, 2022.\\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. Bleu: a method for automatic evaluation of machine\\ntranslation. In Proceedings of the 40th Annual Meeting of the Association for Computational\\nLinguistics, July 6-12, 2002, Philadelphia, PA, USA, pp. 311–318. ACL, 2002. doi: 10.3115/\\n1073083.1073135. URL https://aclanthology.org/P02-1040/.\\nRapidFuzz. python-levenshtein by rapidfuzz. https://github.com/rapidfuzz/\\npython-Levenshtein, 2023.\\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y ., Liu, J., Remez, T., Rapin,\\nJ., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outra-\\ngeously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\\narXiv:1701.06538, 2017.\\nShnitzer, T., Ou, A., Silva, M., Soule, K., Sun, Y ., Solomon, J., Thompson, N., and Yurochkin, M.\\nLarge language model routing with benchmark datasets, 2024. URLhttps://openreview.net/\\nforum?id=LyNsMNNLjY.\\nTeam, G., Anil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai,\\nA. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint\\narXiv:2312.11805, 2023.\\nThe Mosaic Research Team. Introducing dbrx: A new state-of-the-art open llm. 2024. URL\\nhttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm .\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,\\nN., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\\nBhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\\npreprint arXiv:2307.09288, 2023b.\\nWang, H., Polo, F. M., Sun, Y ., Kundu, S., Xing, E., and Yurochkin, M. Fusing models with\\ncomplementary expertise. In The Twelfth International Conference on Learning Representations,\\n2024a. URL https://openreview.net/forum?id=PhMrGCMIRL.\\nWang, L., Xu, W., Lan, Y ., Hu, Z., Lan, Y ., Lee, R. K.-W., and Lim, E.-P. Plan-and-solve prompt-\\ning: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint\\narXiv:2305.04091, 2023.\\nWang, Q., Wang, Z., Su, Y ., Tong, H., and Song, Y . Rethinking the bounds of llm reasoning: Are\\nmulti-agent discussions the key? arXiv preprint arXiv:2402.18272, 2024b.\\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou,\\nD. Self-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171, 2022.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-\\nthought prompting elicits reasoning in large language models. Advances in Neural Information\\nProcessing Systems, 35:24824–24837, 2022.\\n11\\n\\n0.00 0.05 0.10 0.15 0.20 0.25\\nSpearman correlation coefficient\\nQWen1.5-110B\\nQWen1.5-72B\\nWizardLM\\nLlama-3-70B\\nMixtral-8x22B\\ndbrx-instruct\\nAggregator\\nAggregation\\n1st aggregation\\n2nd aggregation\\n3rd aggregation\\n0.00 0.05 0.10 0.15 0.20 0.25\\nSpearman correlation coefficient\\nQWen1.5-110B\\nQWen1.5-72B\\nWizardLM\\nLlama-3-70B\\nMixtral-8x22B\\ndbrx-instruct\\nAggregator\\nAggregation\\n1st aggregation\\n2nd aggregation\\n3rd aggregation\\nFigure 6: (a) Spearman Correlation using TF-IDF similarity; (b) Spearman Correlation using Leven-\\nshtein similarity.\\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empow-\\nering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244,\\n2023a.\\nXu, X., Tao, C., Shen, T., Xu, C., Xu, H., Long, G., and Lou, J.-g. Re-reading improves reasoning in\\nlanguage models. arXiv preprint arXiv:2309.06275, 2023b.\\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y ., and Narasimhan, K. Tree of thoughts:\\nDeliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023a.\\nYao, Y ., Li, Z., and Zhao, H. Beyond chain-of-thought, effective graph-of-thought reasoning in large\\nlanguage models. arXiv preprint arXiv:2305.16582, 2023b.\\nYe, S., Kim, D., Kim, S., Hwang, H., Kim, S., Jo, Y ., Thorne, J., Kim, J., and Seo, M. Flask: Fine-\\ngrained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928,\\n2023.\\nZhang, J., Xu, X., and Deng, S. Exploring collaboration mechanisms for llm agents: A social\\npsychology view. arXiv preprint arXiv:2310.02124, 2023.\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin,\\nX. V ., et al. Opt: Open pre-trained transformer language models. arXiv e-prints, pp. arXiv–2205,\\n2022a.\\nZhang, Z., Zhang, A., Li, M., and Smola, A. Automatic chain of thought prompting in large language\\nmodels. arXiv preprint arXiv:2210.03493, 2022b.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\\nE. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot\\narena. arXiv preprint arXiv:2306.05685, 2023.\\nSupplementary Material\\nA Spearman Correlation using Different Similarity Functions\\nWe present results using TF-IDF-based similarity and Levenshtein similarity when calculating the\\nSpearman correlation. Specifically, within each sample ofn proposed answers, we calculate Spearman\\ncorrelation coefficient between the n similarity scores and the n preference scores determined by the\\nGPT-4-based evaluator. As shown in Figure 6, there is indeed a positive correlation between win rate\\nand both TF-IDF similarity and Levenshtein similarity.\\n12\\n\\nTable 5: Prompt for ranking with LLMs\\nYou are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on\\nthe quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting\\nthe most accurate and human-preferred answers.\\nI require a leaderboard for various large language models. I’ll provide you with prompts given to these models\\nand their corresponding outputs. Your task is to assess these responses, and select the model that produces the\\nbest output from a human perspective.\\n## Instruction\\n{\\n\"instruction\": \"\"\"{instruction}\"\"\",\\n}\\n## Model Outputs\\nHere are the unordered outputs from the models. Each output is associated with a specific model, identified by a\\nunique model identifier.\\n{\\n{\\n\"model_identifier\": \"{identifier_1}\",\\n\"output\": \"\"\"{output_1}\"\"\"\\n},\\n{\\n\"model_identifier\": \"{identifier_2}\",\\n\"output\": \"\"\"{output_2}\"\"\"\\n},\\n{\\n\"model_identifier\": \"{identifier_3}\",\\n\"output\": \"\"\"{output_3}\"\"\"\\n},\\n{\\n\"model_identifier\": \"{identifier_4}\",\\n\"output\": \"\"\"{output_4}\"\"\"\\n},\\n{\\n\"model_identifier\": \"{identifier_5}\",\\n\"output\": \"\"\"{output_5}\"\"\"\\n},\\n{\\n\"model_identifier\": \"{identifier_6}\",\\n\"output\": \"\"\"{output_6}\"\"\"\\n}\\n}\\n## Task\\nEvaluate the models based on the quality and relevance of their outputs, and select the model that generated the\\nbest output. Answer by providing the model identifier of the best model. We will use your output as the name of\\nthe best model, so make sure your output only contains one of the following model identifiers and nothing else\\n(no quotes, no spaces, no new lines, ...).\\n## Best Model Identifier\\nB LLM Ranker\\nThis section introduces the setup of the LLM-Ranker used in this paper. The LLM-Ranker is designed\\nto evaluate and rank the best output generated by some LLMs. Table 5 presents the template for\\nprompting the model during these evaluations. We use this LLM-Ranker to pick the best answer\\namong and use AlpacaEval evaluator to evaluate the best ranked answer.\\n13\\n\\nTable 6: Case: Some models produce high quality answers.\\nRole Preference Content\\nUser - I want to talk about \"Smooth\", a song by Rob Thomas.\\nQwen1.5-110B-Chat 0.35 ... It’s a perfect blend of rock, pop, and Latin music, showcasing\\nSantana’s signature guitar work and Rob Thomas’s soulful vocals ...\\nQwen1.5-72B-Chat 0.00 ...\\nLlama-3-70B-Instruct 0.00 ... it’s a collaboration between Rob Thomas, the lead vocalist of\\nMatchbox Twenty,and Santana, the legendary guitarist ...\\nWizardLM-2-8x22B 0.99 ... It spent 12 weeks at the top of the Billboard Hot 100 chart in the\\nUnited States ... capturing the song’s sultry and energetic vibe ...\\nMixtral-8x22B-Instruct-v0.1 0.00 ...\\ndbrx-instruc 0.00 ...\\nAggregated\\n(Qwen1.5-110B-Chat) 0.99 \"Smooth\" is actually a collaboration between Rob Thomas, who is\\nthe lead singer of the band Matchbox Twenty, and the renowned\\nLatin rock guitarist Carlos Santana ... This iconic song blends rock,\\npop, and Latin music elements, showcasing Santana’s signature\\nguitar work and Rob Thomas’s distinctive vocals ... The song was a\\ncolossal success, spending 12 weeks at the top of the Billboard Hot\\n100 ... The music video, set in a vibrant club scene, complements\\nthe song’s sultry vibe ...\\nC Case Study\\nWe present a case study in this section. Due to the length of the responses generated by all models,\\nwe will only show selected fragments for brevity. To illustrate how the aggregator synthesizes the\\nresponse, we underlined similar expressions between the proposed responses and the aggregated\\nresponse in different colors. We omit the content that all proposed responses have mentioned.\\nTable 6 showcases the responses generated by different proposers. The aggregated response generated\\nby Qwen1.5-110B-Chat reflects a high preference for its own content but also incorporates key points\\nfrom Llama-3-70B-Instruct and WizardLM 8x22B. Notably, GPT-4’s preference score for WizardLM\\n8x22B’s response is 0.99, and the final aggregated answer also achieves a preference score of 0.99.\\nMeanwhile, Table 7 presents another case where none of the proposed responses achieve a high\\nGPT-4 preference score. Despite this, the aggregator successfully identifies and incorporates the\\nstrong points from these responses, achieving a preference score of 0.33.\\nD MATH Task\\nHere, we demonstrate that our approach is applicable to reasoning tasks, such as those in the MATH\\ndataset Hendrycks et al. (2021). The results are presented in Table 8, where we show that our method\\nconsistently enhances accuracy by a significant margin. This indicates that our approach is also\\neffective for this type of task. Notably, our method is complementary to existing reasoning techniques\\nsuch as Chain of Thought Wei et al. (2022) and Self-consistency Wang et al. (2022).\\n14\\n\\nTable 7: Case: all proposed responses are not good enough.\\nRole Preference Content\\nUser - How do you become an author?\\nQwen1.5-110B-Chat 0.00 Becoming an author involves several steps and ... 3. **Choose\\nyour niche**:... 8. **Understand the publishing industry**:... 10.\\n**Self-publishing options**:...\\nQwen1.5-72B-Chat 0.00 ...\\nLlama-3-70B-Instruct 0.16 Becoming an author can be a rewarding and fulfilling career... 1.\\n**Develop a love for reading and writing**:... 10. **Market and\\npromote your work**:... 11. **Continuously improve**:...\\nWizardLM-2-8x22B 0.03 1. ... 4. Finish your work:...\\nMixtral-8x22B-Instruct-v0.1 0.00 ...\\ndbrx-instruc 0.00 ...\\nAggregated\\n(Qwen1.5-110B-Chat) 0.33 Becoming an author involves a dedicated journey... 1. **Cul-\\ntivate a Love for Writing and Reading**:... 4. **Choose Your\\nNiche**:... 5. **Finish Your Work**:... 10. **Self-Publishing**:...\\n11. **Marketing and Promotion**:... 12. **Continuous Learning\\nand Writing**:...\\nTable 8: Results on the MATH task. We evaluate different aggregators, with all six models serving as\\nproposers in each MoA layer.\\nAggregator Layer 1 Layer 2 Layer 3\\nQwen1.5-72B-Chat 0.428 0.526 0.552\\nQwen1.5-110B-Chat 0.500 0.570 0.576\\nWizard 8x22b 0.544 0.574 0.580\\nMixtral-8x22B-Instruct-v0.1 0.282 0.534 0.556\\nLlama-3-70B-Instruct 0.456 0.584 0.578\\ndbrx-instruct 0.314 0.456 0.522\\n15'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pathlib\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def get_PDF_text(file : str):\n",
    "    text = ''\n",
    "\n",
    "    # Read the PDF file and extract text\n",
    "    try:\n",
    "        with Path(file).open(\"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    except Exception as e:\n",
    "        raise f\"Error reading the PDF file: {str(e)}\"\n",
    "\n",
    "        # Check if the PDF has more than ~131,072 characters\n",
    "        # The context lenght limit of the model is 131,072 tokens and thus the text should be less than this limit\n",
    "    if len(text) > 131072:\n",
    "        raise \"The PDF is too long. Please upload a PDF with fewer than ~131072 characters.\"\n",
    "\n",
    "    return text\n",
    "\n",
    "text = get_PDF_text('./MoA.pdf')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Extracted Details and Snippets**\n",
      "\n",
      "* **Key Concepts:**\n",
      "\t+ Mixture-of-Agents (MoA) methodology\n",
      "\t+ Large Language Models (LLMs)\n",
      "\t+ Collaborativeness of LLMs\n",
      "\t+ Proposers and Aggregators\n",
      "* **Research Contributions:**\n",
      "\t+ Novel framework for leveraging multiple LLMs\n",
      "\t+ Finding of collaborativeness among LLMs\n",
      "\t+ State-of-the-art performance on AlpacaEval 2.0, MT-Bench, and FLASK benchmarks\n",
      "* **Methodology:**\n",
      "\t+ MoA architecture consists of multiple layers with multiple LLM agents\n",
      "\t+ Each layer aggregates outputs from previous layer and generates new responses\n",
      "\t+ Aggregators can be specialized or general-purpose models\n",
      "* **Experimental Results:**\n",
      "\t+ MoA outperforms GPT-4 Omni on AlpacaEval 2.0 and FLASK benchmarks\n",
      "\t+ MoA-Lite achieves competitive performance with fewer layers and lower cost\n",
      "\t+ Analysis of Spearman correlation between win rate and similarity scores (BLEU, TF-IDF, Levenshtein)\n",
      "\t+ Case studies demonstrate MoA's ability to synthesize responses from multiple models\n",
      "* **Related Work:**\n",
      "\t+ Previous studies on LLM reasoning, model ensemble, and multi-agent collaboration\n",
      "\t+ Comparison with existing approaches (e.g., PAIR RANKER, GENFUSER)\n",
      "* **Future Directions:**\n",
      "\t+ Optimization of MoA architecture\n",
      "\t+ Exploration of chunk-wise aggregation and latency reduction techniques\n",
      "\t+ Application of MoA to other tasks and domains\n",
      "\n",
      "**Insights and Observations**\n",
      "\n",
      "* The collaborativeness of LLMs is a key phenomenon that can be leveraged to improve response quality.\n",
      "* MoA's ability to synthesize responses from multiple models allows it to outperform individual models.\n",
      "* The choice of aggregator and proposer models is crucial for MoA's performance.\n",
      "* MoA's architecture can be optimized to reduce latency and improve cost-effectiveness.\n",
      "* The findings of this study have implications for the development of more effective and interpretable LLMs.\n"
     ]
    }
   ],
   "source": [
    "source_doc = text\n",
    "\n",
    "extracted_details = run_llm(CLEAN_EXTRACT_DETAILS.format(source_doc=source_doc), \n",
    "                            model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', \n",
    "                            system_prompt=SYSTEM_PROMPT)\n",
    "print(extracted_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Podcast Outline and Brainstormed Ideas: Mixture-of-Agents Enhances Large Language Model Capabilities**\n",
      "\n",
      "**I. Introduction (2-3 minutes)**\n",
      "\n",
      "* Brief overview of Large Language Models (LLMs) and their capabilities\n",
      "* Introduction of the Mixture-of-Agents (MoA) methodology and its potential to enhance LLMs\n",
      "* Thesis statement: MoA can significantly improve the performance of LLMs by leveraging their collective strengths\n",
      "\n",
      "**II. What is MoA and How Does it Work? (10-12 minutes)**\n",
      "\n",
      "* Explanation of the MoA architecture and its components (proposers, aggregators, and layers)\n",
      "* Discussion of how MoA differs from other approaches (e.g., Mixture-of-Experts)\n",
      "* Use analogies or storytelling techniques to make the concept more relatable and accessible\n",
      "\n",
      "**III. The Collaborativeness of LLMs (10-12 minutes)**\n",
      "\n",
      "* Explanation of the collaborativeness phenomenon among LLMs\n",
      "* Discussion of how MoA leverages this phenomenon to improve response quality\n",
      "* Use examples or case studies to illustrate the benefits of collaborativeness\n",
      "\n",
      "**IV. Experimental Results and Evaluation (10-12 minutes)**\n",
      "\n",
      "* Overview of the benchmarks used to evaluate MoA (AlpacaEval 2.0, MT-Bench, FLASK)\n",
      "* Discussion of the results, highlighting MoA's performance compared to other models (e.g., GPT-4 Omni)\n",
      "* Analysis of the cost-effectiveness and latency reduction of MoA\n",
      "\n",
      "**V. What Makes MoA Work Well? (10-12 minutes)**\n",
      "\n",
      "* Discussion of the importance of model diversity and the number of proposers\n",
      "* Analysis of the specialization of models in the MoA ecosystem\n",
      "* Use thought-provoking questions to explore the implications of these findings\n",
      "\n",
      "**VI. Future Directions and Applications (5-7 minutes)**\n",
      "\n",
      "* Discussion of potential future directions for MoA research (e.g., optimization, chunk-wise aggregation)\n",
      "* Exploration of potential applications for MoA (e.g., chat assistants, educational tools)\n",
      "* Final thoughts and conclusions\n",
      "\n",
      "**VII. Conclusion (2-3 minutes)**\n",
      "\n",
      "* Recap of the key points discussed in the podcast\n",
      "* Final thoughts on the potential of MoA to enhance LLMs and its implications for the future of AI\n",
      "\n",
      "**Brainstormed Ideas:**\n",
      "\n",
      "* Use a storytelling approach to introduce the concept of MoA, making it more relatable and accessible to a general audience\n",
      "* Create a hypothetical scenario to illustrate the benefits of collaborativeness among LLMs\n",
      "* Use analogies to explain complex concepts, such as the MoA architecture and its components\n",
      "* Explore the implications of MoA for the future of AI and its potential applications in various fields\n",
      "* Consider inviting a guest expert to discuss the potential applications and limitations of MoA\n",
      "* Use thought-provoking questions to encourage listeners to think critically about the topics discussed in the podcast\n"
     ]
    }
   ],
   "source": [
    "outline = run_llm(OUTLINE_PROMPT.format(extracted_details=extracted_details, source_doc=source_doc),\n",
    "                    model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',\n",
    "                    system_prompt=SYSTEM_PROMPT)\n",
    "\n",
    "print(outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scratchpad': \"Let's explore how to present the key points engagingly: Use storytelling to introduce MoA, making it more relatable and accessible to a general audience. Create a hypothetical scenario to illustrate the benefits of collaborativeness among LLMs. Use analogies to explain complex concepts, such as the MoA architecture and its components. Explore the implications of MoA for the future of AI and its potential applications in various fields.\",\n",
       " 'name_of_guest': 'Dr. Rachel Kim',\n",
       " 'dialogue': [{'speaker': 'Host (Jane)',\n",
       "   'text': \"Welcome to today's podcast, where we're exploring the exciting world of Large Language Models. I'm your host, Jane, and joining me is Dr. Rachel Kim, an expert in AI and the author of a groundbreaking paper on Mixture-of-Agents. Dr. Kim, thanks for being here!\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"Thanks, Jane! I'm thrilled to share my research with your audience.\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"So, let's dive right in. What's this Mixture-of-Agents, or MoA, all about?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': 'Well, MoA is a novel framework that leverages the collective strengths of multiple Large Language Models to improve response quality. Think of it like a team of experts working together to generate more accurate and informative answers.'},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': 'That sounds fascinating. Can you walk us through how MoA works?'},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': 'Sure thing! MoA consists of multiple layers, each comprising multiple LLM agents. These agents can be proposers or aggregators, and they work together to generate new responses. The key is that these agents can be specialized or general-purpose models, allowing for a more diverse range of expertise.'},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"I see. So, it's like a hierarchical structure, where each layer builds on the previous one. But what's the magic behind MoA? What makes it so effective?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"Ah, that's where the collaborativeness of LLMs comes in. Our research shows that LLMs can work together in a way that's greater than the sum of its parts. By leveraging this collaborativeness, MoA can outperform individual models and achieve state-of-the-art results.\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"Wow, that's impressive. Can you share some examples or case studies that demonstrate the benefits of MoA?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"Certainly. We've evaluated MoA on several benchmarks, including AlpacaEval 2.0 and FLASK. The results show that MoA can achieve significant improvements over individual models, even when compared to powerful models like GPT-4 Omni.\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"That's amazing. What about the cost-effectiveness and latency of MoA? How does it compare to other approaches?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"We've found that MoA can be optimized to reduce latency and improve cost-effectiveness. In fact, our MoA-Lite variant achieves competitive performance with fewer layers and lower cost.\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"I'm curious, what makes MoA work so well? Is it the diversity of models, the number of proposers, or something else entirely?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"It's actually a combination of factors. The specialization of models in the MoA ecosystem is crucial, as is the number of proposers. We've also found that the choice of aggregator and proposer models can significantly impact performance.\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"That's really interesting. What about future directions for MoA research? What are some potential applications for this technology?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"We're excited to explore optimization techniques, such as chunk-wise aggregation and latency reduction. We also see potential applications for MoA in chat assistants, educational tools, and more.\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"Well, Dr. Kim, it's been an absolute pleasure having you on the show. Before we wrap up, is there a final thought or call-to-action you'd like to share with our audience?\"},\n",
       "  {'speaker': 'Guest',\n",
       "   'text': \"Thanks, Jane! I'd like to encourage listeners to explore the possibilities of MoA and its potential applications. Who knows what innovative solutions we can create by harnessing the collective strengths of LLMs?\"},\n",
       "  {'speaker': 'Host (Jane)',\n",
       "   'text': \"Thanks, Dr. Kim, for sharing your insights with us today. That's all the time we have for now. Join us next time on the podcast for more exciting conversations about AI and its applications.\"}]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script = JSON_llm(SCRIPT_PROMPT.format(extracted_details=extracted_details, outline=outline),\n",
    "                    Dialogue,\n",
    "                    system_prompt=SYSTEM_PROMPT)\n",
    "\n",
    "script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clean and extract details `given` source text\n",
    "2. Generate an outline `given` extracted information and the source text\n",
    "3. Generate a script `given` the facts from step 1 and outline from step 2\n",
    "4. Call Text to Speech model to generate the Podcast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function that takes a PDF file as input and returns the podcast script.\n",
    "def prompt_chain_podcast_workflow(file : str):\n",
    "    text = get_PDF_text(file)\n",
    "    source_doc = text\n",
    "    \n",
    "    extracted_details = run_llm(CLEAN_EXTRACT_DETAILS.format(source_doc=source_doc), \n",
    "                            model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', \n",
    "                            system_prompt=SYSTEM_PROMPT)\n",
    "    \n",
    "    outline = run_llm(OUTLINE_PROMPT.format(extracted_details=extracted_details, source_doc=source_doc),\n",
    "                    model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',\n",
    "                    system_prompt=SYSTEM_PROMPT)\n",
    "    \n",
    "    script = JSON_llm(SCRIPT_PROMPT.format(extracted_details=extracted_details, outline=outline),\n",
    "                    Dialogue,\n",
    "                    system_prompt=SYSTEM_PROMPT)\n",
    "    return script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate code for a series of llm calls that will generate a podcast script given a PDF file as input.\n",
    "\n",
    "#TO DO Replase with Together API Cartesia model usage\n",
    "\n",
    "import subprocess\n",
    "import ffmpeg\n",
    "\n",
    "host_id = \"694f9389-aac1-45b6-b726-9d9369183238\" # Jane - host\n",
    "guest_id = \"a0e99841-438c-4a64-b679-ae501e7d6091\" # Guest\n",
    "\n",
    "model_id = \"sonic-english\" # The Sonic Cartesia model for English TTS\n",
    "\n",
    "output_format = {\n",
    "    \"container\": \"raw\",\n",
    "    \"encoding\": \"pcm_f32le\",\n",
    "    \"sample_rate\": 44100,\n",
    "    }\n",
    "\n",
    "# Set up a WebSocket connection.\n",
    "ws = client_cartesia.tts.websocket()\n",
    "\n",
    "# Open a file to write the raw PCM audio bytes to.\n",
    "f = open(\"podcast.pcm\", \"wb\")\n",
    "\n",
    "# Generate and stream audio.\n",
    "for line in script.dialogue:\n",
    "    if line.speaker == \"Guest\":\n",
    "        voice_id = guest_id\n",
    "    else:\n",
    "        voice_id = host_id\n",
    "\n",
    "    for output in ws.send(\n",
    "        model_id=model_id,\n",
    "        transcript='-' + line.text, # the \"-\"\" is to add a pause between speakers\n",
    "        voice_id=voice_id,\n",
    "        stream=True,\n",
    "        output_format=output_format,\n",
    "    ):\n",
    "        buffer = output[\"audio\"]  # buffer contains raw PCM audio bytes\n",
    "        f.write(buffer)\n",
    "\n",
    "# Close the connection to release resources\n",
    "ws.close()\n",
    "f.close()\n",
    "\n",
    "# Convert the raw PCM bytes to a WAV file.\n",
    "ffmpeg.input(\"podcast.pcm\", format=\"f32le\").output(\"podcast.wav\").run()\n",
    "\n",
    "# Play the file\n",
    "subprocess.run([\"ffplay\", \"-autoexit\", \"-nodisp\", \"podcast.wav\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Routing Agentic Workflow\n",
    "\n",
    "`Given prompt -> LLM strcutured model choice -> Call this model -> Output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUTER_SYSTEM_PROMPT = \"Choose optimal model\"\n",
    "\n",
    "JSON output from router -> given prompt output -> Model, reason\n",
    "\n",
    "Given model name call that model and pass it the Model selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class ModelOutput(BaseModel):\n",
    "    model: Literal[\"deepseek-ai/DeepSeek-V3\", \n",
    "                   \"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "                   \"Gryphe/MythoMax-L2-13b\", \n",
    "                   \"Qwen/QwQ-32B-Preview\",\n",
    "                   \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"]\n",
    "    reason: str = Field(\n",
    "        description=\"Reason why this model was selected for the task specified in the prompt/query.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Qwen/Qwen2.5-Coder-32B-Instruct',\n",
       " 'reason': 'The task requires code generation, specifically a Python code snippet to check if a number is prime or not, which aligns with the capabilities of the Qwen/Qwen2.5-Coder-32B-Instruct model, making it the best option for this task.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROUTER_SYSTEM_PROMPT = \"\"\"Given a user prompt/query, select the best model from the available options to solve the task and provide a reason for your choice.\n",
    "Each model has different capabilities - select the best model for the task provided:\n",
    "- deepseek-ai/DeepSeek-V3: Good generic model to default to incase no better alternative is selected\n",
    "- Qwen/Qwen2.5-Coder-32B-Instruct: Best for code generation tasks\n",
    "- Gryphe/MythoMax-L2-13b: Best model for story-telling and role-play and fantasy tasks\n",
    "- Qwen/QwQ-32B-Preview: Best model for reasoning, math and muiltistep tasks\n",
    "- meta-llama/Llama-3.3-70B-Instruct-Turbo: Best model for general enterprise usecases and tasks\"\"\"\n",
    "\n",
    "ROUTER_PROMPT = \"Given a user prompt/query: {user_query}, select the best model from the available options to solve the task and provide a reason for your choice. Answer only in JSON format.\"\n",
    "\n",
    "prompt = \"Produce python code snippet to check to see if a number is prime or not.\"\n",
    "\n",
    "selected_model = JSON_llm(ROUTER_PROMPT.format(user_query=prompt),\n",
    "                            ModelOutput,\n",
    "                            system_prompt=ROUTER_SYSTEM_PROMPT)\n",
    "\n",
    "selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! Below is a Python code snippet to check if a number is prime:\\n\\n```python\\ndef is_prime(n):\\n    \"\"\"Check if a number is prime.\"\"\"\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n    i = 5\\n    while i * i <= n:\\n        if n % i == 0 or n % (i + 2) == 0:\\n            return False\\n        i += 6\\n    return True\\n\\n# Example usage:\\nnumber = 29\\nif is_prime(number):\\n    print(f\"{number} is a prime number.\")\\nelse:\\n    print(f\"{number} is not a prime number.\")\\n```\\n\\n### Explanation:\\n1. **Initial Checks**:\\n   - Numbers less than or equal to 1 are not prime.\\n   - Numbers 2 and 3 are prime.\\n   - If the number is divisible by 2 or 3, it is not prime.\\n\\n2. **Loop through potential factors**:\\n   - We start checking from 5 and increment by 6 (i.e., check 5, 11, 17, ...).\\n   - For each number `i`, we check if `n` is divisible by `i` or `i + 2`.\\n   - This works because all primes greater than 3 can be written in the form of 6k ± 1.\\n\\n3. **Efficiency**:\\n   - The loop runs up to the square root of `n`, making it efficient for larger numbers.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = run_llm(user_prompt= prompt, \n",
    "                   model = selected_model['model']\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that will call the router and then the output llm model in sequence to generate a response to a user prompt.\n",
    "def run_router_workflow(user_prompt : str):\n",
    "    \n",
    "    selected_model = JSON_llm(ROUTER_PROMPT.format(user_query=user_prompt),\n",
    "                            ModelOutput,\n",
    "                            system_prompt=ROUTER_SYSTEM_PROMPT)\n",
    "    \n",
    "    response = run_llm(user_prompt= user_prompt, \n",
    "                   model = selected_model['model']\n",
    "    )\n",
    "    return selected_model['model'], selected_model['reason'], response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Produce python code snippet to check to see if a number is prime or not.\n",
      "========================================\n",
      "Selected Model: Qwen/Qwen2.5-Coder-32B-Instruct \n",
      " Reason: The task requires generating Python code to solve a specific problem, which aligns with Qwen2.5-Coder-32B-Instruct's capability as the best model for code generation tasks.\n",
      "========================================\n",
      "Response: Certainly! Below is a Python code snippet that checks if a number is prime:\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    \"\"\"Check if a number is prime.\"\"\"\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "    i = 5\n",
      "    while i * i <= n:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "\n",
      "# Example usage:\n",
      "number = 29\n",
      "if is_prime(number):\n",
      "    print(f\"{number} is a prime number.\")\n",
      "else:\n",
      "    print(f\"{number} is not a prime number.\")\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Initial Checks**:\n",
      "   - Numbers less than or equal to 1 are not prime.\n",
      "   - Numbers 2 and 3 are prime.\n",
      "   - Any even number greater than 2 and any number divisible by 3 greater than 3 are not prime.\n",
      "   \n",
      "2. **Loop through Potential Divisors**:\n",
      "   - Start checking from 5 and increment by 6 (i.e., check 5, 11, 17, etc.).\n",
      "   - This works because all primes greater than 3 are of the form 6k ± 1.\n",
      "   - For each `i`, check if `n` is divisible by `i` or `i + 2`.\n",
      "   - If `i * i` exceeds `n`, then `n` is prime.\n",
      "\n",
      "This method is efficient for checking primality for reasonably large numbers.\n"
     ]
    }
   ],
   "source": [
    "model, reason, response = run_router_workflow(prompt)\n",
    "\n",
    "print(f\"Query: {prompt}\")\n",
    "print(20*'==')\n",
    "print(f\"Selected Model: {model} \\n Reason: {reason}\")\n",
    "print(20*'==')\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parallelization\n",
    "\n",
    "`Mixture of Agents code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\%'\n",
      "/var/folders/yd/r39kvy_94lvg2n0_11qz3pf00000gn/T/ipykernel_28378/823531316.py:8: SyntaxWarning: invalid escape sequence '\\%'\n",
      "  user_prompt = \"\"\"Tim wants to invest some money in a bank which compounds quarterly\n"
     ]
    }
   ],
   "source": [
    "reference_models = [\n",
    "    \"microsoft/WizardLM-2-8x22B\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "]\n",
    "\n",
    "user_prompt = \"\"\"Tim wants to invest some money in a bank which compounds quarterly\n",
    "with an annual interest rate of $7\\%$. To the nearest dollar, how much money should he\n",
    "invest if he wants a total of $\\$60,\\!000$ at the end of $5$ years?\"\"\"\n",
    "\n",
    "# Generate intermediate reference responses\n",
    "results = await asyncio.gather(*[run_llm_parallel(user_prompt=user_prompt, model=model) for model in reference_models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_model = \"deepseek-ai/DeepSeek-V3\"\n",
    "\n",
    "aggregator_system_prompt = \"\"\"You have been provided with a set of responses from various open-source models to the latest user query.\n",
    "Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information\n",
    "provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the\n",
    "given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured,\n",
    "coherent, and adheres to the highest standards of accuracy and reliability.\n",
    "\n",
    "Responses from models:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have been provided with a set of responses from various open-source models to the latest user query.\n",
      "Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information\n",
      "provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the\n",
      "given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured,\n",
      "coherent, and adheres to the highest standards of accuracy and reliability.\n",
      "\n",
      "Responses from models:\n",
      "1.  Let's think step by step.To determine how much money Tim should invest to have $60,000 at the end of 5 years, with a quarterly compounding interest rate of 7%, we can use the formula for compound interest:\n",
      "\n",
      "\\[ A = P \\left(1 + \\frac{r}{n}\\right)^{nt} \\]\n",
      "\n",
      "where:\n",
      "- \\( A \\) is the amount of money accumulated after n years, including interest.\n",
      "- \\( P \\) is the principal amount (the initial amount of money).\n",
      "- \\( r \\) is the annual interest rate (decimal).\n",
      "- \\( n \\) is the number of times that interest is compounded per year.\n",
      "- \\( t \\) is the time the money is invested for, in years.\n",
      "\n",
      "Given:\n",
      "- \\( A = \\$60,000 \\) (the final amount Tim wants to have)\n",
      "- \\( r = 7\\% = 0.07 \\) (annual interest rate as a decimal)\n",
      "- \\( n = 4 \\) (since the interest is compounded quarterly)\n",
      "- \\( t = 5 \\) years (the time period for the investment)\n",
      "\n",
      "We need to solve for \\( P \\) (the initial investment):\n",
      "\n",
      "\\[ P = \\frac{A}{\\left(1 + \\frac{r}{n}\\right)^{nt}} \\]\n",
      "\n",
      "Plugging in the values:\n",
      "\n",
      "\\[ P = \\frac{60,000}{\\left(1 + \\frac{0.07}{4}\\right)^{4 \\cdot 5}} \\]\n",
      "\n",
      "\\[ P = \\frac{60,000}{\\left(1 + 0.0175\\right)^{20}} \\]\n",
      "\n",
      "\\[ P = \\frac{60,000}{\\left(1.0175\\right)^{20}} \\]\n",
      "\n",
      "Now we calculate the value inside the parentheses:\n",
      "\n",
      "\\[ \\left(1.0175\\right)^{20} \\]\n",
      "\n",
      "This is the growth factor over the 5-year period, considering the interest is compounded quarterly.\n",
      "\n",
      "After calculating the growth factor, we divide $60,000 by this number to find the initial investment \\( P \\).\n",
      "\n",
      "Finally, we round the result to the nearest dollar to find out how much Tim should invest.\n",
      "\n",
      "Let's calculate the growth factor and the initial investment:\n",
      "\n",
      "\\[ \\left(1.0175\\right)^{20} \\approx 1.1797081302923 \\]\n",
      "\n",
      "\\[ P = \\frac{60,000}{1.1797081302923} \\approx 50,882.34 \\]\n",
      "\n",
      "Rounding to the nearest dollar:\n",
      "\n",
      "\\[ P \\approx \\$50,882 \\]\n",
      "\n",
      "Therefore, Tim should invest approximately $50,882 to have $60,000 at the end of 5 years, with the interest being compounded quarterly at an annual rate of 7%.\n",
      "\n",
      "The final answer is \\boxed{50882}.\n",
      "\n",
      "The answer is: 50882.\n",
      "2. To determine how much money Tim should invest to achieve a total of $60,000 at the end of 5 years with an annual interest rate of 7% compounded quarterly, we can use the formula for compound interest:\n",
      "\n",
      "\\[ A = P \\left(1 + \\frac{r}{n}\\right)^{nt} \\]\n",
      "\n",
      "where:\n",
      "- \\( A \\) is the amount of money accumulated after n years, including interest.\n",
      "- \\( P \\) is the principal amount (the initial amount of money).\n",
      "- \\( r \\) is the annual interest rate (decimal).\n",
      "- \\( n \\) is the number of times that interest is compounded per year.\n",
      "- \\( t \\) is the time the money is invested for in years.\n",
      "\n",
      "Given:\n",
      "- \\( A = 60,000 \\)\n",
      "- \\( r = 0.07 \\)\n",
      "- \\( n = 4 \\) (since the interest is compounded quarterly)\n",
      "- \\( t = 5 \\)\n",
      "\n",
      "We need to find \\( P \\). Plugging the given values into the formula, we get:\n",
      "\n",
      "\\[ 60,000 = P \\left(1 + \\frac{0.07}{4}\\right)^{4 \\times 5} \\]\n",
      "\n",
      "First, calculate the quarterly interest rate:\n",
      "\n",
      "\\[ \\frac{0.07}{4} = 0.0175 \\]\n",
      "\n",
      "Next, calculate the total number of compounding periods:\n",
      "\n",
      "\\[ 4 \\times 5 = 20 \\]\n",
      "\n",
      "Now, substitute these values back into the formula:\n",
      "\n",
      "\\[ 60,000 = P \\left(1 + 0.0175\\right)^{20} \\]\n",
      "\n",
      "Calculate \\( \\left(1 + 0.0175\\right)^{20} \\):\n",
      "\n",
      "\\[ 1.0175^{20} \\approx 1.414778 \\]\n",
      "\n",
      "So the equation becomes:\n",
      "\n",
      "\\[ 60,000 = P \\times 1.414778 \\]\n",
      "\n",
      "To find \\( P \\), divide both sides by 1.414778:\n",
      "\n",
      "\\[ P = \\frac{60,000}{1.414778} \\approx 42,401.31 \\]\n",
      "\n",
      "Rounding to the nearest dollar, we get:\n",
      "\n",
      "\\[ P \\approx 42,401 \\]\n",
      "\n",
      "Therefore, Tim should invest approximately \\(\\boxed{42,401}\\) dollars to have a total of $60,000 at the end of 5 years.\n",
      "3. Here's how to solve this problem:\n",
      "\n",
      "**Understanding Compound Interest**\n",
      "\n",
      "Compound interest means that interest is calculated not only on the principal amount but also on the accumulated interest from previous periods.\n",
      "\n",
      "**Formula**\n",
      "\n",
      "The formula for compound interest is:\n",
      "\n",
      "A = P(1 + r/n)^(nt)\n",
      "\n",
      "Where:\n",
      "\n",
      "* A = the future value of the investment/loan, including interest\n",
      "* P = the principal investment amount (the initial deposit or loan amount)\n",
      "* r = the annual interest rate (as a decimal)\n",
      "* n = the number of times that interest is compounded per year\n",
      "* t = the number of years the money is invested or borrowed for\n",
      "\n",
      "**Applying the Formula**\n",
      "\n",
      "1. **Identify the knowns:**\n",
      "\n",
      "   * A = $60,000 (the desired future value)\n",
      "   * r = 0.07 (7% annual interest rate)\n",
      "   * n = 4 (compounded quarterly)\n",
      "   * t = 5 years\n",
      "\n",
      "2. **Solve for P (the principal):**\n",
      "\n",
      "   $60,000 = P(1 + 0.07/4)^(4*5)\n",
      "   $60,000 = P(1.0175)^20\n",
      "   P = $60,000 / (1.0175)^20 \n",
      "   P ≈ $42,898 \n",
      "\n",
      "**Answer:**\n",
      "\n",
      "To the nearest dollar, Tim should invest approximately **$42,898**.\n",
      "4. ## Step 1: Understand the problem\n",
      "We are given that Tim wants to invest money in a bank with an annual interest rate of $7\\%$, compounded quarterly, and he wants a total of $\\$60,\\!000$ at the end of $5$ years.\n",
      "\n",
      "## Step 2: Determine the formula for compound interest\n",
      "The formula for compound interest is $A = P \\left(1 + \\frac{r}{n}\\right)^{nt}$, where $A$ is the amount of money accumulated after $n$ years, including interest, $P$ is the principal amount (initial investment), $r$ is the annual interest rate (in decimal), $n$ is the number of times that interest is compounded per year, and $t$ is the time the money is invested for in years.\n",
      "\n",
      "## Step 3: Identify given values and plug them into the formula\n",
      "Given values are $A = \\$60,000$, $r = 7\\% = 0.07$, $n = 4$ (since interest is compounded quarterly), and $t = 5$ years. We need to solve for $P$.\n",
      "\n",
      "## Step 4: Substitute given values into the formula\n",
      "Substituting the given values into the formula, we get $\\$60,000 = P \\left(1 + \\frac{0.07}{4}\\right)^{4 \\cdot 5}$.\n",
      "\n",
      "## Step 5: Simplify the equation\n",
      "Simplifying, we have $\\$60,000 = P \\left(1 + 0.0175\\right)^{20}$, which becomes $\\$60,000 = P \\left(1.0175\\right)^{20}$.\n",
      "\n",
      "## Step 6: Calculate the value of $\\left(1.0175\\right)^{20}$\n",
      "Using a calculator, $\\left(1.0175\\right)^{20} \\approx 1.419067227$.\n",
      "\n",
      "## Step 7: Solve for $P$\n",
      "Now, we solve for $P$: $P = \\frac{\\$60,000}{1.419067227}$.\n",
      "\n",
      "## Step 8: Calculate $P$\n",
      "Calculating $P$, we get $P \\approx \\$42,409.47466$.\n",
      "\n",
      "## Step 9: Round $P$ to the nearest dollar\n",
      "Rounding $P$ to the nearest dollar, we get $P \\approx \\$42,409$.\n",
      "\n",
      "The final answer is: $\\boxed{42409}$\n"
     ]
    }
   ],
   "source": [
    "print(aggregator_system_prompt + \"\\n\" + \"\\n\".join(f\"{i+1}. {str(element)}\" for i, element in enumerate(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how much Tim should invest to have $60,000 at the end of 5 years with an annual interest rate of 7% compounded quarterly, we use the compound interest formula:\n",
      "\n",
      "\\[\n",
      "A = P \\left(1 + \\frac{r}{n}\\right)^{nt}\n",
      "\\]\n",
      "\n",
      "Where:\n",
      "- \\(A\\) is the future amount ($60,000),\n",
      "- \\(P\\) is the principal (the amount to invest),\n",
      "- \\(r\\) is the annual interest rate (0.07),\n",
      "- \\(n\\) is the number of compounding periods per year (4),\n",
      "- \\(t\\) is the time in years (5).\n",
      "\n",
      "We rearrange the formula to solve for \\(P\\):\n",
      "\n",
      "\\[\n",
      "P = \\frac{A}{\\left(1 + \\frac{r}{n}\\right)^{nt}}\n",
      "\\]\n",
      "\n",
      "Substituting the given values:\n",
      "\n",
      "\\[\n",
      "P = \\frac{60,000}{\\left(1 + \\frac{0.07}{4}\\right)^{4 \\cdot 5}}\n",
      "\\]\n",
      "\n",
      "Simplify the equation step by step:\n",
      "\n",
      "1. Calculate the quarterly interest rate:\n",
      "\\[\n",
      "\\frac{0.07}{4} = 0.0175\n",
      "\\]\n",
      "\n",
      "2. Calculate the total number of compounding periods:\n",
      "\\[\n",
      "4 \\times 5 = 20\n",
      "\\]\n",
      "\n",
      "3. Calculate the growth factor:\n",
      "\\[\n",
      "\\left(1 + 0.0175\\right)^{20} \\approx 1.414778\n",
      "\\]\n",
      "\n",
      "4. Solve for \\(P\\):\n",
      "\\[\n",
      "P = \\frac{60,000}{1.414778} \\approx 42,401.31\n",
      "\\]\n",
      "\n",
      "Rounding to the nearest dollar:\n",
      "\n",
      "\\[\n",
      "P \\approx 42,401\n",
      "\\]\n",
      "\n",
      "**Final Answer:** Tim should invest approximately \\(\\boxed{42,\\!401}\\) dollars.\n"
     ]
    }
   ],
   "source": [
    "final_output = run_llm(user_prompt=user_prompt,\n",
    "                       model=aggregator_model,\n",
    "                       system_prompt=aggregator_system_prompt + \"\\n\" + \"\\n\".join(f\"{i+1}. {str(element)}\" for i, element in enumerate(results)\n",
    "           ))\n",
    "\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Orchestrator-workers\n",
    "\n",
    "`Summarization for product descriptions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List\n",
    "\n",
    "class Task(BaseModel):\n",
    "    type: Literal[\"formal\", \"conversational\", \"hybrid\"]\n",
    "    description: str\n",
    "\n",
    "class TaskList(BaseModel):\n",
    "    analysis: str\n",
    "    tasks: List[Task]  = Field(..., default_factory=list)\n",
    "\n",
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "Analyze this task and break it down into 2-3 distinct approaches:\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Provide an Analysis:\n",
    "\n",
    "Explain your understanding of the task and which variations would be valuable.\n",
    "Focus on how each approach serves different aspects of the task.\n",
    "\n",
    "\n",
    "Along with the analysis, provide 2-3 approaches to tackle the task, each with a brief description:\n",
    "\n",
    "Formal style: Write technically and precisely, focusing on detailed specifications\n",
    "Conversational style: Write in a friendly and engaging way that connects with the reader\n",
    "Hybrid style: Tell a story that includes technical details, combining emotional elements with specifications\n",
    "\n",
    "Return only JSON output.\n",
    "\"\"\"\n",
    "\n",
    "WORKER_PROMPT = \"\"\"\n",
    "Generate content based on:\n",
    "Task: {original_task}\n",
    "Style: {task_type}\n",
    "Guidelines: {task_description}\n",
    "\n",
    "Return your response in this format:\n",
    "\n",
    "<response>\n",
    "Your content here, maintaining the specified style and fully addressing requirements.\n",
    "</response>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'Task': {'properties': {'type': {'enum': ['formal',\n",
       "      'conversational',\n",
       "      'hybrid'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'description': {'title': 'Description', 'type': 'string'}},\n",
       "   'required': ['type', 'description'],\n",
       "   'title': 'Task',\n",
       "   'type': 'object'}},\n",
       " 'properties': {'analysis': {'title': 'Analysis', 'type': 'string'},\n",
       "  'tasks': {'items': {'$ref': '#/$defs/Task'},\n",
       "   'title': 'Tasks',\n",
       "   'type': 'array'}},\n",
       " 'required': ['analysis'],\n",
       " 'title': 'TaskList',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskList.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis': \"The task requires writing a product description for an eco-friendly water bottle targeting environmentally conscious millennials. The key features of the product are its plastic-free composition, insulation, and lifetime warranty. To effectively cater to this audience, it's essential to consider different approaches that highlight the product's unique selling points while resonating with the target audience's values and preferences.\",\n",
       " 'tasks': [{'type': 'formal',\n",
       "   'description': \"This approach focuses on providing detailed technical specifications, emphasizing the product's eco-friendly materials, insulation properties, and warranty. It's ideal for readers seeking in-depth information about the product's features and benefits.\"},\n",
       "  {'type': 'conversational',\n",
       "   'description': \"This approach takes a friendly and engaging tone, connecting with the reader on an emotional level. It highlights how the product aligns with the target audience's values, such as reducing plastic waste and living sustainably. This style is perfect for building brand loyalty and encouraging readers to take action.\"},\n",
       "  {'type': 'hybrid',\n",
       "   'description': \"This approach combines technical details with storytelling elements, creating a compelling narrative that showcases the product's features and benefits. It appeals to both logical and emotional aspects of the reader, making it an effective way to convey the product's value proposition and unique selling points.\"}]}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = \"\"\"Write a product description for a new eco-friendly water bottle. \n",
    "The target_audience is environmentally conscious millennials and key product features are: plastic-free, insulated, lifetime warranty\n",
    "\"\"\"\n",
    "\n",
    "orchestrator_response = JSON_llm(ORCHESTRATOR_PROMPT.format(task=task),\n",
    "                                 schema=TaskList\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ORCHESTRATOR OUTPUT ===\n",
      "\n",
      "ANALYSIS:\n",
      "The task requires writing a product description for an eco-friendly water bottle targeting environmentally conscious millennials. The key features of the product are its plastic-free composition, insulation, and lifetime warranty. To effectively cater to this audience, it's essential to consider different approaches that highlight the product's unique selling points while resonating with the target audience's values and preferences.\n",
      "\n",
      "TASKS:\n",
      "[\n",
      "  {\n",
      "    \"type\": \"formal\",\n",
      "    \"description\": \"This approach focuses on providing detailed technical specifications, emphasizing the product's eco-friendly materials, insulation properties, and warranty. It's ideal for readers seeking in-depth information about the product's features and benefits.\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"conversational\",\n",
      "    \"description\": \"This approach takes a friendly and engaging tone, connecting with the reader on an emotional level. It highlights how the product aligns with the target audience's values, such as reducing plastic waste and living sustainably. This style is perfect for building brand loyalty and encouraging readers to take action.\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"hybrid\",\n",
      "    \"description\": \"This approach combines technical details with storytelling elements, creating a compelling narrative that showcases the product's features and benefits. It appeals to both logical and emotional aspects of the reader, making it an effective way to convey the product's value proposition and unique selling points.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Parse orchestrator response\n",
    "analysis = orchestrator_response[\"analysis\"]\n",
    "tasks= orchestrator_response[\"tasks\"]\n",
    "        \n",
    "print(\"\\n=== ORCHESTRATOR OUTPUT ===\")\n",
    "print(f\"\\nANALYSIS:\\n{analysis}\")\n",
    "print(f\"\\nTASKS:\\n{json.dumps(tasks, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Process each task\n",
    "task = \"\"\"Write a product description for a new eco-friendly water bottle. \n",
    "The target_audience is environmentally conscious millennials and key product features are: plastic-free, insulated, lifetime warranty\n",
    "\"\"\"\n",
    "\n",
    "reference_models = [\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"]*len(tasks)\n",
    "\n",
    "worker_response = await asyncio.gather(*[run_llm_parallel(user_prompt=WORKER_PROMPT.format(original_task=task, task_type=task_info['type'], task_description=task_info['description']), model=model) \\\n",
    "                                         for task_info, model in zip(tasks,reference_models)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<response>\\nIntroducing the EcoHydrate water bottle, a revolutionary, eco-friendly hydration solution designed specifically for environmentally conscious individuals. Constructed from high-quality, BPA-free stainless steel (18/8 food-grade), this bottle is completely plastic-free, aligning with the values of reducing plastic waste and promoting sustainability.\\n\\nThe EcoHydrate water bottle boasts an impressive insulation capability, thanks to its double-walled vacuum insulation technology. This advanced design ensures that beverages maintain their temperature for hours, whether hot or cold, making it an ideal companion for daily commutes, outdoor adventures, or fitness activities.\\n\\nWith a generous capacity of 27 ounces (800 ml), this water bottle is spacious enough to meet your hydration needs throughout the day, while its compact dimensions (10.5 inches tall, 3.5 inches wide) make it easily portable and convenient to carry.\\n\\nThe EcoHydrate water bottle has undergone rigorous testing and has earned prestigious certifications, including ISO 9001:2015 for quality management and ISO 14001:2015 for environmental management. Additionally, it complies with FDA and EU food contact regulations, guaranteeing the safety and non-toxicity of the materials used.\\n\\nWhat sets the EcoHydrate apart is its lifetime warranty, a testament to the manufacturer's confidence in the product's durability and performance. This warranty ensures that customers can enjoy their bottle for years to come, without worrying about defects or premature wear.\\n\\nTechnical Specifications:\\n- Material: 18/8 food-grade stainless steel\\n- Capacity: 27 ounces (800 ml)\\n- Dimensions: 10.5 inches (26.7 cm) tall, 3.5 inches (8.9 cm) wide\\n- Weight: 13.5 ounces (382 grams)\\n- Insulation: Double-walled vacuum insulation\\n- Certifications: ISO 9001:2015, ISO 14001:2015, FDA, and EU food contact compliant\\n- Warranty: Lifetime warranty\\n\\nFor environmentally conscious millennials seeking a reliable, eco-friendly water bottle that combines functionality, durability, and sustainability, the EcoHydrate is the perfect choice. Its plastic-free design, advanced insulation, and lifetime warranty make it an investment in both personal health and the health of the planet.\\n</response>\",\n",
       " \"<response>\\nHey there, fellow eco-warriors! Are you tired of contributing to the staggering amount of plastic waste that's harming our planet? Do you want to stay hydrated on-the-go without sacrificing your values? Look no further! We've got the perfect solution for you - our brand new, eco-friendly water bottle that's about to become your new best friend.\\n\\nMade from 100% plastic-free materials, our water bottle is designed to help you reduce your carbon footprint and do your part for the planet. But that's not all - it's also insulated, keeping your drinks hot or cold for hours, whether you're hiking, biking, or just running errands. And with our lifetime warranty, you can trust that your bottle will be by your side for many adventures to come.\\n\\nWe know that staying hydrated is essential, especially when you're living an active lifestyle. That's why we created a bottle that's not only eco-friendly but also designed for convenience. The insulated design ensures that your drinks stay at the perfect temperature, and the durable construction can withstand even the toughest conditions.\\n\\nBy choosing our eco-friendly water bottle, you're not only getting a high-quality product that will last you a lifetime, but you're also joining a movement to reduce plastic waste and protect our planet. Every small step counts, and making the switch to a reusable water bottle is one of the simplest and most effective ways to make a positive impact.\\n\\nSo why wait? Join the thousands of like-minded individuals who have already made the switch to our eco-friendly water bottle. Together, we can make a difference and create a more sustainable future for our planet. Get your hands on our amazing water bottle today and start sipping your way to a greener tomorrow!\\n</response>\",\n",
       " \"<response>\\nImagine a world where staying hydrated doesn't have to come at the cost of the planet. Our new eco-friendly water bottle is more than just a product - it's a symbol of a movement towards a more sustainable future. As a team of environmentally conscious individuals, we embarked on a journey to design a water bottle that not only keeps your drinks hot or cold for hours but also helps reduce the staggering amount of plastic waste that ends up in our oceans and landfills.\\n\\nThe design process was meticulous, with every detail carefully considered to ensure that our product would meet the high standards of our target audience: environmentally conscious millennials who demand both style and substance. We chose to use a unique blend of stainless steel and silicone, making our bottle completely plastic-free. The double-walled insulation keeps your drinks at the perfect temperature, whether you're sipping hot coffee on a chilly morning or enjoying a refreshing glass of ice-cold water on a summer hike.\\n\\nBut what really sets our product apart is its lifetime warranty. We're so confident in the quality of our materials and craftsmanship that we're willing to stand behind our product for as long as you own it. This means that you can enjoy your favorite drinks without worrying about the environmental impact of constantly replacing disposable water bottles. In fact, by choosing our eco-friendly water bottle, you'll be saving up to 156 plastic bottles per year from entering the waste stream.\\n\\nThe impact of our product on the environment is significant. By switching to a reusable water bottle, you'll be reducing your carbon footprint and helping to conserve natural resources. Our bottle is also designed to be recyclable at the end of its life cycle, ensuring that it won't contribute to the staggering amount of waste that ends up in our oceans and landfills.\\n\\nBut our commitment to sustainability doesn't stop there. We're also partnering with environmental organizations to support initiatives that promote recycling, reduce waste, and protect our planet's natural resources. With every purchase of our eco-friendly water bottle, you'll be supporting a movement towards a more sustainable future.\\n\\nSo why settle for a ordinary water bottle when you can have an extraordinary one that not only keeps your drinks hot or cold but also helps save the planet? Join the movement towards a more sustainable future with our new eco-friendly water bottle. With its sleek design, advanced insulation, and lifetime warranty, it's the perfect choice for anyone who cares about the planet and wants to make a positive impact. \\n</response>\"]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WORKER RESULT (formal) ===\n",
      "<response>\n",
      "Introducing the EcoHydrate water bottle, a revolutionary, eco-friendly hydration solution designed specifically for environmentally conscious individuals. Constructed from high-quality, BPA-free stainless steel (18/8 food-grade), this bottle is completely plastic-free, aligning with the values of reducing plastic waste and promoting sustainability.\n",
      "\n",
      "The EcoHydrate water bottle boasts an impressive insulation capability, thanks to its double-walled vacuum insulation technology. This advanced design ensures that beverages maintain their temperature for hours, whether hot or cold, making it an ideal companion for daily commutes, outdoor adventures, or fitness activities.\n",
      "\n",
      "With a generous capacity of 27 ounces (800 ml), this water bottle is spacious enough to meet your hydration needs throughout the day, while its compact dimensions (10.5 inches tall, 3.5 inches wide) make it easily portable and convenient to carry.\n",
      "\n",
      "The EcoHydrate water bottle has undergone rigorous testing and has earned prestigious certifications, including ISO 9001:2015 for quality management and ISO 14001:2015 for environmental management. Additionally, it complies with FDA and EU food contact regulations, guaranteeing the safety and non-toxicity of the materials used.\n",
      "\n",
      "What sets the EcoHydrate apart is its lifetime warranty, a testament to the manufacturer's confidence in the product's durability and performance. This warranty ensures that customers can enjoy their bottle for years to come, without worrying about defects or premature wear.\n",
      "\n",
      "Technical Specifications:\n",
      "- Material: 18/8 food-grade stainless steel\n",
      "- Capacity: 27 ounces (800 ml)\n",
      "- Dimensions: 10.5 inches (26.7 cm) tall, 3.5 inches (8.9 cm) wide\n",
      "- Weight: 13.5 ounces (382 grams)\n",
      "- Insulation: Double-walled vacuum insulation\n",
      "- Certifications: ISO 9001:2015, ISO 14001:2015, FDA, and EU food contact compliant\n",
      "- Warranty: Lifetime warranty\n",
      "\n",
      "For environmentally conscious millennials seeking a reliable, eco-friendly water bottle that combines functionality, durability, and sustainability, the EcoHydrate is the perfect choice. Its plastic-free design, advanced insulation, and lifetime warranty make it an investment in both personal health and the health of the planet.\n",
      "</response>\n",
      "\n",
      "\n",
      "=== WORKER RESULT (conversational) ===\n",
      "<response>\n",
      "Hey there, fellow eco-warriors! Are you tired of contributing to the staggering amount of plastic waste that's harming our planet? Do you want to stay hydrated on-the-go without sacrificing your values? Look no further! We've got the perfect solution for you - our brand new, eco-friendly water bottle that's about to become your new best friend.\n",
      "\n",
      "Made from 100% plastic-free materials, our water bottle is designed to help you reduce your carbon footprint and do your part for the planet. But that's not all - it's also insulated, keeping your drinks hot or cold for hours, whether you're hiking, biking, or just running errands. And with our lifetime warranty, you can trust that your bottle will be by your side for many adventures to come.\n",
      "\n",
      "We know that staying hydrated is essential, especially when you're living an active lifestyle. That's why we created a bottle that's not only eco-friendly but also designed for convenience. The insulated design ensures that your drinks stay at the perfect temperature, and the durable construction can withstand even the toughest conditions.\n",
      "\n",
      "By choosing our eco-friendly water bottle, you're not only getting a high-quality product that will last you a lifetime, but you're also joining a movement to reduce plastic waste and protect our planet. Every small step counts, and making the switch to a reusable water bottle is one of the simplest and most effective ways to make a positive impact.\n",
      "\n",
      "So why wait? Join the thousands of like-minded individuals who have already made the switch to our eco-friendly water bottle. Together, we can make a difference and create a more sustainable future for our planet. Get your hands on our amazing water bottle today and start sipping your way to a greener tomorrow!\n",
      "</response>\n",
      "\n",
      "\n",
      "=== WORKER RESULT (hybrid) ===\n",
      "<response>\n",
      "Imagine a world where staying hydrated doesn't have to come at the cost of the planet. Our new eco-friendly water bottle is more than just a product - it's a symbol of a movement towards a more sustainable future. As a team of environmentally conscious individuals, we embarked on a journey to design a water bottle that not only keeps your drinks hot or cold for hours but also helps reduce the staggering amount of plastic waste that ends up in our oceans and landfills.\n",
      "\n",
      "The design process was meticulous, with every detail carefully considered to ensure that our product would meet the high standards of our target audience: environmentally conscious millennials who demand both style and substance. We chose to use a unique blend of stainless steel and silicone, making our bottle completely plastic-free. The double-walled insulation keeps your drinks at the perfect temperature, whether you're sipping hot coffee on a chilly morning or enjoying a refreshing glass of ice-cold water on a summer hike.\n",
      "\n",
      "But what really sets our product apart is its lifetime warranty. We're so confident in the quality of our materials and craftsmanship that we're willing to stand behind our product for as long as you own it. This means that you can enjoy your favorite drinks without worrying about the environmental impact of constantly replacing disposable water bottles. In fact, by choosing our eco-friendly water bottle, you'll be saving up to 156 plastic bottles per year from entering the waste stream.\n",
      "\n",
      "The impact of our product on the environment is significant. By switching to a reusable water bottle, you'll be reducing your carbon footprint and helping to conserve natural resources. Our bottle is also designed to be recyclable at the end of its life cycle, ensuring that it won't contribute to the staggering amount of waste that ends up in our oceans and landfills.\n",
      "\n",
      "But our commitment to sustainability doesn't stop there. We're also partnering with environmental organizations to support initiatives that promote recycling, reduce waste, and protect our planet's natural resources. With every purchase of our eco-friendly water bottle, you'll be supporting a movement towards a more sustainable future.\n",
      "\n",
      "So why settle for a ordinary water bottle when you can have an extraordinary one that not only keeps your drinks hot or cold but also helps save the planet? Join the movement towards a more sustainable future with our new eco-friendly water bottle. With its sleek design, advanced insulation, and lifetime warranty, it's the perfect choice for anyone who cares about the planet and wants to make a positive impact. \n",
      "</response>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print wokrer resulst sjson dumps nicely\n",
    "worker_results = []\n",
    "\n",
    "for task_info, response in zip(tasks, worker_response):\n",
    "    worker_results.append({\n",
    "                \"type\": task_info[\"type\"],\n",
    "                \"description\": task_info[\"description\"],\n",
    "                \"result\": response\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n=== WORKER RESULT ({task_info['type']}) ===\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluator-optimizer\n",
    "\n",
    "`Code generation and evaluation in a loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(BaseModel):\n",
    "    evaluation: Literal[\"PASS\", \"NEEDS_IMPROVEMENT\", \"FAIL\"]\n",
    "    feedback: str\n",
    "\n",
    "evaluator_prompt = \"\"\"\n",
    "Evaluate this following code implementation for:\n",
    "1. code correctness\n",
    "2. time complexity\n",
    "3. style and best practices\n",
    "\n",
    "You should be evaluating only and not attemping to solve the task.\n",
    "\n",
    "Only output \"PASS\" if all criteria are met and you have no further suggestions for improvements.\n",
    "\n",
    "Provide detailed feedback if there are areas that need improvement. You should specify what needs improvement and why.\n",
    "\n",
    "Only output JSON.\n",
    "\"\"\"\n",
    "\n",
    "generator_prompt = \"\"\"\n",
    "Your goal is to complete the task based on <user input>. If there are feedback \n",
    "from your previous generations, you should reflect on them to improve your solution\n",
    "\n",
    "Output your answer concisely in the following format: \n",
    "\n",
    "Thoughts:\n",
    "[Your understanding of the task and feedback and how you plan to improve]\n",
    "\n",
    "Response:\n",
    "[Your code implementation here]\n",
    "\"\"\"\n",
    "\n",
    "task = \"\"\"\n",
    "Implement a Stack with:\n",
    "1. push(x)\n",
    "2. pop()\n",
    "3. getMin()\n",
    "All operations should be O(1).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n{context}\\nTask: {task}\" if context else f\"{prompt}\\nTask: {task}\"\n",
    "    response = run_llm(full_prompt, model=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "    \n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "    print(f\"Output:\\n{response}\\n\")\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "def evaluate(prompt: str, content: str, task: str, schema) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\nOriginal task: {task}\\nContent to evaluate: {content}\"\n",
    "    response = JSON_llm(full_prompt, schema)\n",
    "    evaluation = response[\"evaluation\"]\n",
    "    feedback = response[\"feedback\"]\n",
    "\n",
    "    print(\"=== EVALUATION START ===\")\n",
    "    print(f\"Status: {evaluation}\")\n",
    "    print(f\"Feedback: {feedback}\")\n",
    "    print(\"=== EVALUATION END ===\\n\")\n",
    "\n",
    "    return evaluation, feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
    "    memory = []\n",
    "    \n",
    "    response = generate(generator_prompt, task)\n",
    "    memory.append(response)\n",
    "\n",
    "    while True:\n",
    "        evaluation, feedback = evaluate(evaluator_prompt, response, task, Evaluation)\n",
    "        if evaluation == \"PASS\":\n",
    "            return response\n",
    "            \n",
    "        context = \"\\n\".join([\n",
    "            \"Previous attempts:\",\n",
    "            *[f\"- {m}\" for m in memory],\n",
    "            f\"\\nFeedback: {feedback}\"\n",
    "        ])\n",
    "        \n",
    "        response = generate(generator_prompt, task, context)\n",
    "        memory.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "To implement a stack with all operations (push, pop, getMin) in O(1) time complexity, we need to use an auxiliary data structure to keep track of the minimum values. A common approach is to use a second stack that stores the minimum values alongside the main stack. This way, every time we push a new element, we also push the current minimum onto the auxiliary stack. When we pop an element, we pop from both stacks. The top of the auxiliary stack will always be the minimum element of the main stack.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        if self.stack:\n",
      "            if self.stack[-1] == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "            self.stack.pop()\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        if self.min_stack:\n",
      "            return self.min_stack[-1]\n",
      "        return None  # or raise an exception if the stack is empty\n",
      "```\n",
      "\n",
      "This implementation ensures that all operations are performed in O(1) time complexity. The `min_stack` keeps track of the minimum values, allowing `getMin` to return the minimum in constant time.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: code_correctness: PASS, time_complexity: PASS, style_and_best_practices: FAIL, suggestions: \"The documentation and error handling could be improved. For example, in the getMin method, returning None for an empty stack might not be the best approach, as it could lead to errors if the caller is not expecting None. Consider raising an exception instead. Additionally, some methods could benefit from docstrings to explain their purpose and behavior. Consider adding type hints for the return types of the methods. The variable names could be more descriptive. For example, 'x' in the push method could be 'value' or 'element'.\"\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "Based on the feedback, I will improve the code by adding docstrings, type hints, and more descriptive variable names. Additionally, I will raise exceptions instead of returning `None` for the `getMin` method when the stack is empty.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, value: int) -> None:\n",
      "        \"\"\"Push a new element onto the stack.\"\"\"\n",
      "        self.stack.append(value)\n",
      "        if not self.min_stack or value <= self.min_stack[-1]:\n",
      "            self.min_stack.append(value)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        \"\"\"Remove the top element from the stack.\"\"\"\n",
      "        if self.stack:\n",
      "            if self.stack[-1] == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "            self.stack.pop()\n",
      "        else:\n",
      "            raise IndexError(\"pop from empty stack\")\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        \"\"\"Return the minimum element in the stack.\"\"\"\n",
      "        if self.min_stack:\n",
      "            return self.min_stack[-1]\n",
      "        raise IndexError(\"getMin from empty stack\")\n",
      "```\n",
      "\n",
      "This implementation maintains the O(1) time complexity for all operations while improving code clarity and robustness.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: PASS\n",
      "Feedback: Code is correct, efficient and follows best practices.\n",
      "=== EVALUATION END ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thoughts:\\nBased on the feedback, I will improve the code by adding docstrings, type hints, and more descriptive variable names. Additionally, I will raise exceptions instead of returning `None` for the `getMin` method when the stack is empty.\\n\\nResponse:\\n```python\\nclass MinStack:\\n    def __init__(self):\\n        self.stack = []\\n        self.min_stack = []\\n\\n    def push(self, value: int) -> None:\\n        \"\"\"Push a new element onto the stack.\"\"\"\\n        self.stack.append(value)\\n        if not self.min_stack or value <= self.min_stack[-1]:\\n            self.min_stack.append(value)\\n\\n    def pop(self) -> None:\\n        \"\"\"Remove the top element from the stack.\"\"\"\\n        if self.stack:\\n            if self.stack[-1] == self.min_stack[-1]:\\n                self.min_stack.pop()\\n            self.stack.pop()\\n        else:\\n            raise IndexError(\"pop from empty stack\")\\n\\n    def getMin(self) -> int:\\n        \"\"\"Return the minimum element in the stack.\"\"\"\\n        if self.min_stack:\\n            return self.min_stack[-1]\\n        raise IndexError(\"getMin from empty stack\")\\n```\\n\\nThis implementation maintains the O(1) time complexity for all operations while improving code clarity and robustness.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop(task, evaluator_prompt, generator_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
